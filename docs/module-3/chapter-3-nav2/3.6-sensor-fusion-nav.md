---
title: Sensor Fusion for Navigation
description: Advanced techniques for fusing multiple sensors to enhance navigation capabilities in humanoid robots
sidebar_position: 6
---

# 3.6 Sensor Fusion for Navigation

## Overview

Sensor fusion is a critical component of robust navigation systems for humanoid robots, combining data from multiple sensors to create a comprehensive and accurate understanding of the environment. Unlike wheeled robots that primarily rely on 2D sensors, humanoid robots must navigate in 3D environments with complex balance requirements, making sensor fusion even more important for safe and efficient navigation.

The challenge in humanoid navigation lies in combining diverse sensor modalities that provide different types of information: LiDAR for precise distance measurements, cameras for visual context and texture information, IMUs for orientation and acceleration, and force/torque sensors for ground contact feedback. The fusion process must account for the different characteristics, noise patterns, and update rates of these sensors.

## Sensor Modalities for Humanoid Navigation

### LiDAR Sensors

LiDAR sensors provide accurate distance measurements and are fundamental for obstacle detection and mapping in humanoid navigation.

**Advantages:**
- High accuracy in distance measurements
- Consistent performance in various lighting conditions
- Fast update rates (10-20 Hz typical)
- Good range (5-30m depending on model)

**Limitations:**
- Limited height information (typically 2D or sparse 3D)
- Difficulty detecting transparent or highly reflective surfaces
- Susceptible to interference from other LiDARs
- Can miss thin obstacles (ropes, wires)

**Configuration for humanoid navigation:**
```yaml
lidar_sensor:
  ros__parameters:
    topic: /scan
    sensor_frame: base_laser_link
    data_type: LaserScan
    max_obstacle_height: 2.0  # Humanoid can step over small obstacles
    obstacle_max_range: 5.0   # Effective range for navigation
    obstacle_min_range: 0.1   # Avoid extremely close readings
    raytrace_max_range: 6.0   # Clear space beyond obstacles
    raytrace_min_range: 0.0   # Clear from sensor position
    inf_is_valid: false       # Invalid readings treated as free space
    clearing: true            # Clear space beyond max range
    marking: true             # Mark detected obstacles
```

### Camera Systems

Cameras provide rich visual information for navigation, including texture, color, and semantic understanding of the environment.

**Advantages:**
- Rich visual context for environment understanding
- Ability to detect and classify objects
- Semantic information for decision making
- Good for texture and color-based navigation

**Limitations:**
- Performance affected by lighting conditions
- Requires significant computational resources
- Depth estimation challenges
- Limited range compared to LiDAR

**Stereo camera configuration:**
```yaml
stereo_camera:
  ros__parameters:
    left_topic: /stereo/left/image_rect
    right_topic: /stereo/right/image_rect
    left_camera_frame: stereo_left_camera_frame
    right_camera_frame: stereo_right_camera_frame
    queue_size: 5
    max_height: 2.0
    min_height: 0.0
    max_obstacle_height: 1.8  # Above which obstacles are ignored
    obstacle_range: 3.0       # Range for obstacle detection
    raytrace_range: 4.0       # Range for raytracing
    vertical_fov: 60.0        # Vertical field of view in degrees
    horizontal_fov: 90.0      # Horizontal field of view in degrees
```

### Inertial Measurement Units (IMU)

IMUs provide crucial information about the robot's orientation and acceleration, essential for balance and navigation in humanoid robots.

**Advantages:**
- High-frequency measurements (100-1000 Hz)
- Direct measurement of orientation and angular velocity
- Essential for balance control
- Not affected by external conditions

**Limitations:**
- Drift over time (especially for orientation)
- Noise in acceleration measurements
- Requires careful calibration
- Integration errors accumulate

**IMU configuration:**
```yaml
imu_sensor:
  ros__parameters:
    topic: /imu/data
    sensor_frame: imu_link
    update_rate: 100.0
    orientation_variance: [0.01, 0.01, 0.01]  # Variance in roll, pitch, yaw
    angular_velocity_variance: [0.001, 0.001, 0.001]
    linear_acceleration_variance: [0.01, 0.01, 0.01]
    use_magnetic_field: false  # Magnetic field can be unreliable indoors
```

### Force/Torque Sensors

Force/torque sensors in the feet provide critical feedback for balance and ground contact detection in humanoid robots.

**Advantages:**
- Direct measurement of ground contact forces
- Essential for balance control
- Detection of ground compliance
- Feedback for step planning

**Limitations:**
- Only provide information at contact points
- Can be affected by robot dynamics
- Limited environmental information
- Require precise calibration

## Sensor Fusion Architectures

### Kalman Filter-Based Fusion

Kalman filters provide an optimal solution for fusing sensors with different characteristics and noise patterns.

```cpp
// Extended Kalman Filter for humanoid state estimation
class HumanoidEKF
{
public:
    HumanoidEKF()
    {
        // Initialize state vector: [x, y, theta, vx, vy, vtheta, zmp_x, zmp_y]
        state_.resize(8, 0.0);

        // Initialize covariance matrix
        covariance_ = Eigen::MatrixXd::Identity(8, 8);
        covariance_ *= 1.0;  // Initial uncertainty

        // Process noise covariance
        process_noise_ = Eigen::MatrixXd::Identity(8, 8);
        process_noise_(0,0) = 0.1;   // x position uncertainty
        process_noise_(1,1) = 0.1;   // y position uncertainty
        process_noise_(2,2) = 0.05;  // theta orientation uncertainty
        process_noise_(3,3) = 0.5;   // x velocity uncertainty
        process_noise_(4,4) = 0.5;   // y velocity uncertainty
        process_noise_(5,5) = 0.3;   // theta velocity uncertainty
        process_noise_(6,6) = 0.2;   // ZMP x uncertainty
        process_noise_(7,7) = 0.2;   // ZMP y uncertainty
    }

    void predict(const geometry_msgs::msg::Twist & control_input, double dt)
    {
        // State transition model for humanoid navigation
        // Predict next state based on control input and dynamics
        Eigen::VectorXd new_state = state_transition_model(state_, control_input, dt);

        // Jacobian of state transition model
        Eigen::MatrixXd F = calculate_jacobian(state_, control_input, dt);

        // Predict covariance
        covariance_ = F * covariance_ * F.transpose() + process_noise_;

        state_ = new_state;
    }

    void update_lidar(const sensor_msgs::msg::LaserScan & scan)
    {
        // Extract features from LiDAR data (obstacles, walls, etc.)
        std::vector<Eigen::Vector2d> features = extract_features_from_scan(scan);

        for (const auto & feature : features) {
            // Calculate expected measurement
            Eigen::Vector2d expected_measurement = predict_measurement(feature);

            // Measurement Jacobian
            Eigen::MatrixXd H = calculate_measurement_jacobian(feature);

            // Innovation covariance
            Eigen::MatrixXd innovation_cov = H * covariance_ * H.transpose() +
                                           measurement_noise_lidar_;

            // Kalman gain
            Eigen::MatrixXd K = covariance_ * H.transpose() *
                               innovation_cov.inverse();

            // Update state and covariance
            Eigen::Vector2d innovation = feature - expected_measurement;
            state_ = state_ + K * innovation;
            covariance_ = (Eigen::MatrixXd::Identity(8, 8) - K * H) * covariance_;
        }
    }

    void update_imu(const sensor_msgs::msg::Imu & imu_msg)
    {
        // Extract orientation from IMU
        tf2::Quaternion quat(
            imu_msg.orientation.x,
            imu_msg.orientation.y,
            imu_msg.orientation.z,
            imu_msg.orientation.w
        );

        double roll, pitch, yaw;
        tf2::Matrix3x3(quat).getRPY(roll, pitch, yaw);

        // Create measurement vector [theta, omega_z]
        Eigen::VectorXd measurement(2);
        measurement(0) = yaw;
        measurement(1) = imu_msg.angular_velocity.z;

        // Expected measurement based on current state
        Eigen::VectorXd expected_measurement(2);
        expected_measurement(0) = state_(2);  // theta from state
        expected_measurement(1) = state_(5);  // theta velocity from state

        // Measurement Jacobian
        Eigen::MatrixXd H = Eigen::MatrixXd::Zero(2, 8);
        H(0, 2) = 1.0;  // Partial derivative of theta measurement w.r.t. theta state
        H(1, 5) = 1.0;  // Partial derivative of omega measurement w.r.t. omega state

        // Innovation covariance
        Eigen::MatrixXd innovation_cov = H * covariance_ * H.transpose() +
                                       measurement_noise_imu_;

        // Kalman gain
        Eigen::MatrixXd K = covariance_ * H.transpose() *
                           innovation_cov.inverse();

        // Update state and covariance
        Eigen::VectorXd innovation = measurement - expected_measurement;
        state_ = state_ + K * innovation;
        covariance_ = (Eigen::MatrixXd::Identity(8, 8) - K * H) * covariance_;
    }

private:
    Eigen::VectorXd state_;
    Eigen::MatrixXd covariance_;
    Eigen::MatrixXd process_noise_;
    Eigen::MatrixXd measurement_noise_lidar_;
    Eigen::MatrixXd measurement_noise_imu_;

    Eigen::VectorXd state_transition_model(const Eigen::VectorXd & state,
                                          const geometry_msgs::msg::Twist & control,
                                          double dt)
    {
        // Implement the humanoid dynamics model
        // This is a simplified model - real implementation would be more complex
        Eigen::VectorXd new_state = state;

        // Update position based on velocity
        new_state(0) += state(3) * dt;  // x position
        new_state(1) += state(4) * dt;  // y position
        new_state(2) += state(5) * dt;  // theta orientation

        // Update velocity based on control input (simplified)
        new_state(3) += control.linear.x * dt;  // x velocity
        new_state(4) += control.linear.y * dt;  // y velocity
        new_state(5) += control.angular.z * dt; // theta velocity

        return new_state;
    }

    Eigen::MatrixXd calculate_jacobian(const Eigen::VectorXd & state,
                                     const geometry_msgs::msg::Twist & control,
                                     double dt)
    {
        // Calculate Jacobian of state transition model
        Eigen::MatrixXd F = Eigen::MatrixXd::Identity(8, 8);

        // Partial derivatives of state transition model
        // F[i][j] = partial derivative of state i with respect to state j
        F(0, 3) = dt;  // dx/dvx
        F(1, 4) = dt;  // dy/dvy
        F(2, 5) = dt;  // dtheta/dvtheta

        return F;
    }

    Eigen::Vector2d predict_measurement(const Eigen::Vector2d & feature)
    {
        // Predict where this feature should appear in measurements
        // based on current state
        return Eigen::Vector2d::Zero();
    }

    Eigen::MatrixXd calculate_measurement_jacobian(const Eigen::Vector2d & feature)
    {
        // Calculate measurement Jacobian H
        return Eigen::MatrixXd::Zero(2, 8);
    }
};
```

### Particle Filter Approach

Particle filters are particularly useful for humanoid navigation where the state space might be multimodal or non-Gaussian.

```cpp
// Particle filter for humanoid navigation
class HumanoidParticleFilter
{
public:
    struct Particle {
        geometry_msgs::msg::Pose pose;
        double weight;
        geometry_msgs::msg::Twist velocity;
        double balance_score;

        Particle() : weight(1.0), balance_score(1.0) {}
    };

    HumanoidParticleFilter(int num_particles = 1000)
        : num_particles_(num_particles)
    {
        particles_.resize(num_particles_);

        // Initialize particles randomly around initial estimate
        initialize_particles();
    }

    void predict(const geometry_msgs::msg::Twist & control_input, double dt)
    {
        for (auto & particle : particles_) {
            // Apply motion model with noise
            geometry_msgs::msg::Twist noisy_control = add_motion_noise(control_input);

            // Update particle pose based on control input
            update_particle_pose(particle.pose, noisy_control, dt);
        }
    }

    void update(const std::vector<sensor_msgs::msg::LaserScan> & sensor_data)
    {
        for (auto & particle : particles_) {
            // Calculate likelihood based on sensor observations
            double likelihood = calculate_likelihood(particle, sensor_data);

            // Incorporate balance constraints
            double balance_factor = calculate_balance_factor(particle);

            // Update particle weight
            particle.weight *= likelihood * balance_factor;
        }

        // Normalize weights
        normalize_weights();

        // Resample if necessary
        if (effective_particles() < num_particles_ / 2) {
            resample();
        }
    }

    geometry_msgs::msg::Pose get_best_estimate()
    {
        // Find particle with highest weight
        auto best_particle = std::max_element(
            particles_.begin(), particles_.end(),
            [](const Particle & a, const Particle & b) {
                return a.weight < b.weight;
            }
        );

        return best_particle->pose;
    }

private:
    std::vector<Particle> particles_;
    int num_particles_;

    void initialize_particles()
    {
        // Initialize particles around initial estimate with some uncertainty
        for (auto & particle : particles_) {
            // Add random noise to initial pose
            particle.pose.position.x += gaussian_random(0.0, 0.1);
            particle.pose.position.y += gaussian_random(0.0, 0.1);

            // Random orientation
            tf2::Quaternion quat;
            quat.setRPY(0, 0, gaussian_random(0.0, 0.1));
            particle.pose.orientation.x = quat.x();
            particle.pose.orientation.y = quat.y();
            particle.pose.orientation.z = quat.z();
            particle.pose.orientation.w = quat.w();

            particle.weight = 1.0 / num_particles_;
        }
    }

    geometry_msgs::msg::Twist add_motion_noise(const geometry_msgs::msg::Twist & control)
    {
        geometry_msgs::msg::Twist noisy_control = control;

        // Add Gaussian noise to linear and angular velocities
        noisy_control.linear.x += gaussian_random(0.0, 0.05);
        noisy_control.linear.y += gaussian_random(0.0, 0.05);
        noisy_control.angular.z += gaussian_random(0.0, 0.03);

        return noisy_control;
    }

    void update_particle_pose(geometry_msgs::msg::Pose & pose,
                             const geometry_msgs::msg::Twist & control, double dt)
    {
        // Update pose based on control input
        pose.position.x += control.linear.x * dt * cos(tf2::getYaw(pose.orientation));
        pose.position.y += control.linear.x * dt * sin(tf2::getYaw(pose.orientation));

        // Update orientation
        tf2::Quaternion quat(pose.orientation.x, pose.orientation.y,
                           pose.orientation.z, pose.orientation.w);
        tf2::Matrix3x3 mat(quat);
        double roll, pitch, yaw;
        mat.getRPY(roll, pitch, yaw);

        yaw += control.angular.z * dt;

        quat.setRPY(roll, pitch, yaw);
        pose.orientation.x = quat.x();
        pose.orientation.y = quat.y();
        pose.orientation.z = quat.z();
        pose.orientation.w = quat.w();
    }

    double calculate_likelihood(const Particle & particle,
                              const std::vector<sensor_msgs::msg::LaserScan> & sensor_data)
    {
        double likelihood = 1.0;

        // Compare particle's expected sensor readings with actual readings
        for (const auto & scan : sensor_data) {
            // Calculate how well the particle's pose explains the sensor data
            double scan_likelihood = calculate_scan_likelihood(particle, scan);
            likelihood *= scan_likelihood;
        }

        return likelihood;
    }

    double calculate_balance_factor(const Particle & particle)
    {
        // Calculate factor based on how well the particle's pose maintains balance
        // This could consider ZMP position, CoM position, etc.
        return 1.0; // Simplified for example
    }

    void normalize_weights()
    {
        double total_weight = 0.0;
        for (const auto & particle : particles_) {
            total_weight += particle.weight;
        }

        if (total_weight > 0.0) {
            for (auto & particle : particles_) {
                particle.weight /= total_weight;
            }
        }
    }

    int effective_particles()
    {
        double sum_weights_sq = 0.0;
        for (const auto & particle : particles_) {
            sum_weights_sq += particle.weight * particle.weight;
        }

        return sum_weights_sq > 0.0 ? 1.0 / sum_weights_sq : 0;
    }

    void resample()
    {
        std::vector<Particle> new_particles;
        new_particles.reserve(num_particles_);

        // Resample particles based on their weights
        std::random_device rd;
        std::mt19937 gen(rd());
        std::uniform_real_distribution<> dis(0.0, 1.0);

        double cumulative_weight = 0.0;
        size_t current_particle = 0;

        for (int i = 0; i < num_particles_; ++i) {
            cumulative_weight += dis(gen);

            while (cumulative_weight > particles_[current_particle].weight) {
                cumulative_weight -= particles_[current_particle].weight;
                current_particle = (current_particle + 1) % num_particles_;
            }

            new_particles.push_back(particles_[current_particle]);
            new_particles.back().weight = 1.0 / num_particles_;
        }

        particles_ = std::move(new_particles);
    }

    double gaussian_random(double mean, double stddev)
    {
        static std::random_device rd;
        static std::mt19937 gen(rd());
        static std::normal_distribution<> dis(0.0, 1.0);

        return mean + stddev * dis(gen);
    }
};
```

## Multi-Sensor Integration Strategies

### Late Fusion

Late fusion combines sensor data after each sensor's processing pipeline has produced its own interpretation.

```cpp
// Late fusion approach
class LateFusionNavigator
{
public:
    LateFusionNavigator()
    {
        lidar_processor_ = std::make_unique<LidarProcessor>();
        vision_processor_ = std::make_unique<VisionProcessor>();
        imu_processor_ = std::make_unique<IMUProcessor>();

        fusion_engine_ = std::make_unique<DecisionFusionEngine>();
    }

    NavigationDecision fuse_sensors(const SensorDataBundle & sensor_data)
    {
        // Process each sensor modality independently
        auto lidar_result = lidar_processor_->process(sensor_data.lidar);
        auto vision_result = vision_processor_->process(sensor_data.vision);
        auto imu_result = imu_processor_->process(sensor_data.imu);

        // Combine the results at the decision level
        return fusion_engine_->fuse_decisions({
            lidar_result, vision_result, imu_result
        });
    }

private:
    std::unique_ptr<LidarProcessor> lidar_processor_;
    std::unique_ptr<VisionProcessor> vision_processor_;
    std::unique_ptr<IMUProcessor> imu_processor_;
    std::unique_ptr<DecisionFusionEngine> fusion_engine_;
};
```

### Early Fusion

Early fusion combines raw sensor data before processing to create a unified representation.

```cpp
// Early fusion approach
class EarlyFusionNavigator
{
public:
    EarlyFusionNavigator()
    {
        unified_map_ = std::make_unique<UnifiedEnvironmentMap>();
        sensor_fusion_ = std::make_unique<SensorFusionProcessor>();
    }

    bool update_environment_map(const SensorDataBundle & sensor_data)
    {
        // Transform all sensor data to a common coordinate frame
        auto transformed_lidar = transform_to_map_frame(sensor_data.lidar, "lidar");
        auto transformed_vision = transform_to_map_frame(sensor_data.vision, "camera");
        auto transformed_imu = transform_to_map_frame(sensor_data.imu, "imu");

        // Fuse all sensor data into a unified representation
        return sensor_fusion_->fuse_to_map(
            unified_map_,
            {transformed_lidar, transformed_vision, transformed_imu}
        );
    }

private:
    std::unique_ptr<UnifiedEnvironmentMap> unified_map_;
    std::unique_ptr<SensorFusionProcessor> sensor_fusion_;

    SensorData transform_to_map_frame(const SensorData & sensor_data,
                                     const std::string & sensor_type)
    {
        // Transform sensor data to map coordinate frame
        // Implementation depends on sensor type and transformation requirements
        return sensor_data; // Simplified for example
    }
};
```

### Deep Learning-Based Fusion

Modern approaches use neural networks to learn optimal fusion strategies.

```cpp
// Deep learning-based sensor fusion
class DeepFusionNavigator
{
public:
    DeepFusionNavigator()
    {
        // Load pre-trained fusion network
        fusion_network_ = load_fusion_network("humanoid_fusion_model.pt");
    }

    NavigationCommand deep_fuse(const SensorDataBundle & sensor_data)
    {
        // Preprocess sensor data for the neural network
        auto input_tensor = preprocess_sensors(sensor_data);

        // Run through fusion network
        auto output_tensor = fusion_network_.forward(input_tensor);

        // Post-process network output to navigation commands
        return postprocess_output(output_tensor);
    }

private:
    torch::jit::script::Module fusion_network_;

    torch::Tensor preprocess_sensors(const SensorDataBundle & sensor_data)
    {
        // Convert sensor data to tensor format
        // This would involve normalizing, reshaping, and concatenating data
        return torch::zeros({1, 100}); // Simplified for example
    }

    NavigationCommand postprocess_output(const torch::Tensor & output)
    {
        // Convert network output to navigation commands
        auto output_values = output.accessor<float, 2>();

        NavigationCommand cmd;
        cmd.linear_velocity = output_values[0][0];
        cmd.angular_velocity = output_values[0][1];
        cmd.confidence = output_values[0][2];

        return cmd;
    }
};
```

## Humanoid-Specific Fusion Considerations

### Balance-Integrated Fusion

Humanoid navigation fusion must consider balance constraints at every step:

```cpp
// Balance-aware sensor fusion
class BalanceAwareFusion
{
public:
    struct FusionResult {
        geometry_msgs::msg::Pose estimated_pose;
        double balance_confidence;
        std::vector<double> sensor_confidences;
        bool is_stable;
    };

    FusionResult fuse_with_balance(const SensorDataBundle & sensor_data,
                                  const BalanceState & current_balance)
    {
        // Perform standard sensor fusion
        auto base_result = perform_sensor_fusion(sensor_data);

        // Evaluate balance implications of the fused estimate
        auto balance_evaluation = evaluate_balance_implications(
            base_result.estimated_pose, current_balance
        );

        FusionResult result;
        result.estimated_pose = base_result.estimated_pose;
        result.balance_confidence = balance_evaluation.confidence;
        result.is_stable = balance_evaluation.is_stable;
        result.sensor_confidences = base_result.sensor_confidences;

        // If balance is compromised, adjust the estimate
        if (!result.is_stable) {
            result.estimated_pose = adjust_for_balance(
                result.estimated_pose, current_balance
            );
        }

        return result;
    }

private:
    struct BalanceEvaluation {
        double confidence;
        bool is_stable;
        double zmp_deviation;
        double com_drift;
    };

    BalanceEvaluation evaluate_balance_implications(
        const geometry_msgs::msg::Pose & pose,
        const BalanceState & current_balance)
    {
        BalanceEvaluation eval;

        // Calculate ZMP position based on estimated pose
        auto zmp_position = calculate_zmp_from_pose(pose);

        // Calculate CoM position and velocity
        auto com_state = calculate_com_state(pose);

        // Evaluate balance metrics
        eval.zmp_deviation = calculate_zmp_deviation(zmp_position, current_balance.support_polygon);
        eval.com_drift = calculate_com_drift(com_state, current_balance.com_reference);

        // Determine stability
        eval.is_stable = (eval.zmp_deviation < MAX_ZMP_DEVIATION &&
                         eval.com_drift < MAX_COM_DRIFT);

        // Calculate confidence based on sensor reliability and balance
        eval.confidence = calculate_balance_confidence(eval, current_balance);

        return eval;
    }

    geometry_msgs::msg::Pose adjust_for_balance(
        const geometry_msgs::msg::Pose & original_pose,
        const BalanceState & current_balance)
    {
        geometry_msgs::msg::Pose adjusted_pose = original_pose;

        // Adjust pose to maintain balance while staying close to original estimate
        // This might involve modifying position or orientation
        auto zmp_position = calculate_zmp_from_pose(adjusted_pose);

        if (is_zmp_unstable(zmp_position, current_balance.support_polygon)) {
            // Adjust to bring ZMP back to stable region
            adjusted_pose = adjust_pose_for_stability(
                adjusted_pose, current_balance.support_polygon
            );
        }

        return adjusted_pose;
    }

    static const double MAX_ZMP_DEVIATION = 0.05;  // 5cm from support polygon
    static const double MAX_COM_DRIFT = 0.03;      // 3cm from reference
};
```

### Multi-Contact Fusion

For humanoid robots that might use hands for support, fusion must consider multi-contact scenarios:

```cpp
// Multi-contact sensor fusion
class MultiContactFusion
{
public:
    struct ContactState {
        geometry_msgs::msg::Point left_foot;
        geometry_msgs::msg::Point right_foot;
        geometry_msgs::msg::Point left_hand;
        geometry_msgs::msg::Point right_hand;
        bool left_foot_contact;
        bool right_foot_contact;
        bool left_hand_contact;
        bool right_hand_contact;
    };

    geometry_msgs::msg::Pose fuse_with_contacts(
        const SensorDataBundle & sensor_data,
        const ContactState & contacts)
    {
        // Create support polygon based on contact points
        auto support_polygon = create_support_polygon(contacts);

        // Perform fusion considering the support polygon
        auto base_estimate = perform_sensor_fusion(sensor_data);

        // Validate estimate against support polygon
        if (!is_estimate_valid_with_contacts(base_estimate, support_polygon)) {
            // Adjust estimate to be consistent with contact constraints
            return adjust_estimate_for_contacts(base_estimate, support_polygon);
        }

        return base_estimate;
    }

private:
    std::vector<geometry_msgs::msg::Point> create_support_polygon(
        const ContactState & contacts)
    {
        std::vector<geometry_msgs::msg::Point> polygon;

        // Add points where robot is in contact with ground
        if (contacts.left_foot_contact) {
            polygon.push_back(contacts.left_foot);
        }
        if (contacts.right_foot_contact) {
            polygon.push_back(contacts.right_foot);
        }
        if (contacts.left_hand_contact) {
            polygon.push_back(contacts.left_hand);
        }
        if (contacts.right_hand_contact) {
            polygon.push_back(contacts.right_hand);
        }

        // Calculate convex hull of contact points
        return compute_convex_hull(polygon);
    }

    bool is_estimate_valid_with_contacts(
        const geometry_msgs::msg::Pose & estimate,
        const std::vector<geometry_msgs::msg::Point> & support_polygon)
    {
        // Check if the estimated CoM projection is within the support polygon
        geometry_msgs::msg::Point2D com_projection;
        com_projection.x = estimate.position.x;
        com_projection.y = estimate.position.y;

        return is_point_in_polygon(com_projection, support_polygon);
    }

    geometry_msgs::msg::Pose adjust_estimate_for_contacts(
        const geometry_msgs::msg::Pose & original_estimate,
        const std::vector<geometry_msgs::msg::Point> & support_polygon)
    {
        geometry_msgs::msg::Pose adjusted_estimate = original_estimate;

        // Move the estimated position to be within the support polygon
        geometry_msgs::msg::Point2D original_projection;
        original_projection.x = original_estimate.position.x;
        original_projection.y = original_estimate.position.y;

        if (!is_point_in_polygon(original_projection, support_polygon)) {
            // Find the closest point in the support polygon
            auto closest_point = find_closest_point_in_polygon(
                original_projection, support_polygon
            );

            adjusted_estimate.position.x = closest_point.x;
            adjusted_estimate.position.y = closest_point.y;
        }

        return adjusted_estimate;
    }

    std::vector<geometry_msgs::msg::Point> compute_convex_hull(
        const std::vector<geometry_msgs::msg::Point> & points)
    {
        // Implement convex hull algorithm (Graham scan, Jarvis march, etc.)
        // Simplified for example
        return points;
    }

    bool is_point_in_polygon(
        const geometry_msgs::msg::Point2D & point,
        const std::vector<geometry_msgs::msg::Point> & polygon)
    {
        // Implement point-in-polygon test (ray casting, winding number, etc.)
        return false; // Simplified for example
    }
};
```

## Fusion for Different Navigation Scenarios

### Indoor Navigation Fusion

Indoor environments present specific challenges that require specialized fusion strategies:

```yaml
# Indoor navigation sensor fusion configuration
indoor_fusion_config:
  ros__parameters:
    # Sensor weights for indoor environments
    lidar_weight: 0.6          # LiDAR is very reliable indoors
    vision_weight: 0.3         # Vision useful for semantic understanding
    imu_weight: 0.4           # IMU important for orientation
    odometry_weight: 0.5      # Wheel/leg odometry for position tracking

    # Indoor-specific parameters
    max_range_indoor: 10.0    # Typical indoor range
    min_height_obstacle: 0.1  # Minimum obstacle height to consider
    max_height_ceiling: 3.0   # Maximum height to consider
    wall_detection_threshold: 0.1  # Threshold for wall detection

    # Indoor environment assumptions
    static_environment: true   # Assume most objects are static
    lighting_stability: true   # Indoor lighting is generally stable
    magnetic_field_reliability: false  # Magnetic field may be unreliable indoors
```

### Outdoor Navigation Fusion

Outdoor environments require different fusion strategies due to varying conditions:

```yaml
# Outdoor navigation sensor fusion configuration
outdoor_fusion_config:
  ros__parameters:
    # Sensor weights for outdoor environments
    lidar_weight: 0.5         # LiDAR performance may vary
    vision_weight: 0.6        # Vision more important for outdoor
    imu_weight: 0.5           # IMU remains important
    gps_weight: 0.4           # GPS for global positioning
    odometry_weight: 0.3      # Odometry may be less reliable

    # Outdoor-specific parameters
    max_range_outdoor: 20.0   # Extended range for outdoor
    weather_adaptation: true  # Adapt to weather conditions
    terrain_classification: true  # Classify different terrains

    # Outdoor environment considerations
    lighting_adaptation: true # Adapt to varying lighting
    dynamic_environment: true # More dynamic obstacles expected
    gps_integration: true     # Integrate GPS for global positioning
```

## Fusion Quality Assessment

### Confidence-Based Fusion

Assessing the quality of fused data is crucial for safe navigation:

```cpp
// Confidence assessment for sensor fusion
class FusionConfidenceAssessor
{
public:
    struct FusionQuality {
        double overall_confidence;
        std::map<std::string, double> sensor_confidences;
        std::vector<std::string> unreliable_sensors;
        bool fusion_quality_acceptable;
        double consistency_score;
    };

    FusionQuality assess_fusion_quality(
        const std::vector<SensorData> & sensor_inputs,
        const geometry_msgs::msg::Pose & fused_estimate)
    {
        FusionQuality quality;

        // Calculate individual sensor confidences
        quality.sensor_confidences = calculate_sensor_confidences(sensor_inputs);

        // Calculate consistency between sensors
        quality.consistency_score = calculate_consistency(sensor_inputs);

        // Calculate overall confidence
        quality.overall_confidence = calculate_overall_confidence(
            quality.sensor_confidences, quality.consistency_score
        );

        // Identify unreliable sensors
        quality.unreliable_sensors = identify_unreliable_sensors(
            quality.sensor_confidences
        );

        // Determine if fusion quality is acceptable
        quality.fusion_quality_acceptable =
            quality.overall_confidence >= MIN_ACCEPTABLE_CONFIDENCE &&
            quality.consistency_score >= MIN_ACCEPTABLE_CONSISTENCY;

        return quality;
    }

    geometry_msgs::msg::Pose adapt_fusion_based_on_quality(
        const FusionQuality & quality,
        const geometry_msgs::msg::Pose & current_estimate,
        const std::vector<SensorData> & sensor_inputs)
    {
        geometry_msgs::msg::Pose adapted_estimate = current_estimate;

        if (!quality.fusion_quality_acceptable) {
            // Reduce reliance on low-confidence sensors
            adapted_estimate = apply_quality_adaptation(
                quality, current_estimate, sensor_inputs
            );
        }

        return adapted_estimate;
    }

private:
    static const double MIN_ACCEPTABLE_CONFIDENCE = 0.6;
    static const double MIN_ACCEPTABLE_CONSISTENCY = 0.5;

    std::map<std::string, double> calculate_sensor_confidences(
        const std::vector<SensorData> & sensor_inputs)
    {
        std::map<std::string, double> confidences;

        for (const auto & sensor_data : sensor_inputs) {
            double confidence = calculate_sensor_confidence(sensor_data);
            confidences[sensor_data.sensor_type] = confidence;
        }

        return confidences;
    }

    double calculate_sensor_confidence(const SensorData & sensor_data)
    {
        // Calculate confidence based on sensor data quality
        // Consider factors like signal strength, noise level, etc.
        return 0.8; // Simplified for example
    }

    double calculate_consistency(const std::vector<SensorData> & sensor_inputs)
    {
        // Calculate how consistent the sensor readings are with each other
        // This might involve comparing estimates from different sensors
        return 0.7; // Simplified for example
    }

    std::vector<std::string> identify_unreliable_sensors(
        const std::map<std::string, double> & confidences)
    {
        std::vector<std::string> unreliable;

        for (const auto & pair : confidences) {
            if (pair.second < SENSOR_CONFIDENCE_THRESHOLD) {
                unreliable.push_back(pair.first);
            }
        }

        return unreliable;
    }

    geometry_msgs::msg::Pose apply_quality_adaptation(
        const FusionQuality & quality,
        const geometry_msgs::msg::Pose & current_estimate,
        const std::vector<SensorData> & sensor_inputs)
    {
        // Apply adaptation based on quality assessment
        // This might involve reducing weight of unreliable sensors
        // or reverting to more conservative estimates
        return current_estimate; // Simplified for example
    }

    static const double SENSOR_CONFIDENCE_THRESHOLD = 0.4;
};
```

## Implementation in Nav2

### Custom Fusion Layer

Implementing a custom fusion layer for Nav2:

```cpp
// Custom sensor fusion layer for Nav2
class SensorFusionLayer : public nav2_costmap_2d::Layer
{
public:
    SensorFusionLayer() = default;
    ~SensorFusionLayer() = default;

    void onInitialize() override
    {
        ros::NodeHandle nh("~/" + name_);

        // Initialize fusion parameters
        nh.param("fusion_method", fusion_method_, std::string("kalman"));
        nh.param("lidar_topic", lidar_topic_, std::string("/scan"));
        nh.param("camera_topic", camera_topic_, std::string("/camera/depth"));
        nh.param("imu_topic", imu_topic_, std::string("/imu/data"));

        // Subscribe to sensor topics
        lidar_sub_ = nh.subscribe(lidar_topic_, 10,
                                 &SensorFusionLayer::lidarCallback, this);
        camera_sub_ = nh.subscribe(camera_topic_, 10,
                                  &SensorFusionLayer::cameraCallback, this);
        imu_sub_ = nh.subscribe(imu_topic_, 10,
                               &SensorFusionLayer::imuCallback, this);

        current_ = true;
        matchSize();
    }

    void updateBounds(double robot_x, double robot_y, double robot_yaw,
                     double* min_x, double* min_y, double* max_x, double* max_y) override
    {
        std::lock_guard<std::mutex> lock(fusion_mutex_);

        // Update bounds based on fused sensor data
        updateWithOverwrite(*min_x, *min_y, *max_x, *max_y);
    }

    void updateCosts(nav2_costmap_2d::Costmap2D& master_grid,
                    int min_i, int min_j, int max_i, int max_j) override
    {
        if (!enabled_) return;

        std::lock_guard<std::mutex> lock(fusion_mutex_);

        // Apply fused sensor data to the master grid
        for (int j = min_j; j < max_j; j++) {
            for (int i = min_i; i < max_i; i++) {
                int index = getIndex(i, j);
                unsigned char old_cost = master_grid.getCost(i, j);

                if (old_cost == nav2_costmap_2d::NO_INFORMATION) continue;

                // Get fused obstacle probability at this location
                double fused_probability = getFusedObstacleProbability(i, j);

                // Convert probability to cost
                unsigned char fused_cost = probabilityToCost(fused_probability);

                // Combine with existing cost
                unsigned char new_cost = combineCosts(old_cost, fused_cost);
                master_grid.setCost(i, j, new_cost);
            }
        }
    }

private:
    std::string fusion_method_;
    std::string lidar_topic_;
    std::string camera_topic_;
    std::string imu_topic_;

    ros::Subscriber lidar_sub_;
    ros::Subscriber camera_sub_;
    ros::Subscriber imu_sub_;

    std::mutex fusion_mutex_;

    // Sensor data storage
    sensor_msgs::LaserScan::ConstPtr latest_lidar_;
    sensor_msgs::Image::ConstPtr latest_camera_;
    sensor_msgs::Imu::ConstPtr latest_imu_;

    void lidarCallback(const sensor_msgs::LaserScan::ConstPtr& msg)
    {
        std::lock_guard<std::mutex> lock(fusion_mutex_);
        latest_lidar_ = msg;
    }

    void cameraCallback(const sensor_msgs::Image::ConstPtr& msg)
    {
        std::lock_guard<std::mutex> lock(fusion_mutex_);
        latest_camera_ = msg;
    }

    void imuCallback(const sensor_msgs::Imu::ConstPtr& msg)
    {
        std::lock_guard<std::mutex> lock(fusion_mutex_);
        latest_imu_ = msg;
    }

    double getFusedObstacleProbability(int i, int j)
    {
        // Convert grid coordinates to world coordinates
        double world_x, world_y;
        layered_costmap_->getCostmap()->mapToWorld(i, j, world_x, world_y);

        // Perform sensor fusion at this location
        if (fusion_method_ == "kalman") {
            return kalmanFusionAtLocation(world_x, world_y);
        } else if (fusion_method_ == "particle") {
            return particleFusionAtLocation(world_x, world_y);
        } else {
            return simpleFusionAtLocation(world_x, world_y);
        }
    }

    double kalmanFusionAtLocation(double x, double y)
    {
        // Perform Kalman filter-based fusion at the given location
        // This would involve predicting sensor measurements and fusing them
        return 0.5; // Simplified for example
    }

    double probabilityToCost(double probability)
    {
        // Convert obstacle probability to costmap value
        if (probability < 0.1) return nav2_costmap_2d::FREE_SPACE;
        if (probability > 0.9) return nav2_costmap_2d::LETHAL_OBSTACLE;

        // Linear mapping between 0.1 and 0.9 to cost values 1-252
        double scaled_prob = (probability - 0.1) / 0.8;
        return static_cast<unsigned char>(1 + scaled_prob * 251);
    }

    unsigned char combineCosts(unsigned char cost1, unsigned char cost2)
    {
        // Combine two cost values (e.g., using max, weighted average, etc.)
        return std::max(cost1, cost2);
    }
};
```

## Performance Optimization

### Efficient Fusion Algorithms

Optimizing fusion algorithms for real-time humanoid navigation:

```cpp
// Efficient fusion with optimized data structures
class EfficientFusionEngine
{
public:
    EfficientFusionEngine()
    {
        // Pre-allocate memory for common operations
        preallocated_weights_.resize(MAX_SENSORS);
        preallocated_measurements_.resize(MAX_MEASUREMENTS);
        preallocated_fused_state_.resize(STATE_DIMENSION);

        // Initialize lookup tables for common calculations
        initialize_lookup_tables();
    }

    inline SensorFusionResult fast_fuse(const SensorBundle & sensors)
    {
        // Fast fusion using pre-allocated memory and optimized algorithms
        int num_valid_sensors = 0;

        // Quickly validate sensor data
        for (int i = 0; i < sensors.size(); ++i) {
            if (is_sensor_valid(sensors[i])) {
                preallocated_weights_[num_valid_sensors] = calculate_sensor_weight(sensors[i]);
                num_valid_sensors++;
            }
        }

        // Normalize weights
        normalize_weights(preallocated_weights_.data(), num_valid_sensors);

        // Perform weighted fusion
        return perform_weighted_fusion(sensors,
                                     preallocated_weights_.data(),
                                     num_valid_sensors);
    }

private:
    static const int MAX_SENSORS = 10;
    static const int MAX_MEASUREMENTS = 100;
    static const int STATE_DIMENSION = 6;

    std::vector<double> preallocated_weights_;
    std::vector<double> preallocated_measurements_;
    std::vector<double> preallocated_fused_state_;

    inline bool is_sensor_valid(const SensorData & sensor)
    {
        // Fast validation using bit flags or simple checks
        return sensor.timestamp > 0 && sensor.data_quality > MIN_QUALITY_THRESHOLD;
    }

    inline double calculate_sensor_weight(const SensorData & sensor)
    {
        // Fast weight calculation using lookup tables
        int quality_idx = static_cast<int>(sensor.data_quality * 100);
        quality_idx = std::min(quality_idx, 99);

        return sensor_quality_weights_[quality_idx];
    }

    void normalize_weights(double * weights, int count)
    {
        double sum = 0.0;
        for (int i = 0; i < count; ++i) {
            sum += weights[i];
        }

        if (sum > 0.0) {
            for (int i = 0; i < count; ++i) {
                weights[i] /= sum;
            }
        }
    }

    SensorFusionResult perform_weighted_fusion(const SensorBundle & sensors,
                                             const double * weights,
                                             int count)
    {
        // Perform the actual fusion operation
        SensorFusionResult result;

        // Weighted average of position estimates
        for (int i = 0; i < count; ++i) {
            result.position.x += sensors[i].position.x * weights[i];
            result.position.y += sensors[i].position.y * weights[i];
            // ... continue for other dimensions
        }

        return result;
    }

    void initialize_lookup_tables()
    {
        // Initialize lookup tables for fast calculations
        for (int i = 0; i < 100; ++i) {
            sensor_quality_weights_[i] = calculate_quality_weight(i / 100.0);
        }
    }

    double calculate_quality_weight(double quality)
    {
        // Calculate weight based on sensor quality
        // This could be a non-linear function
        return quality * quality;  // Quadratic weighting
    }

    double sensor_quality_weights_[100];
    static const double MIN_QUALITY_THRESHOLD = 0.1;
};
```

## Testing and Validation

### Fusion Performance Metrics

Evaluating the effectiveness of sensor fusion:

```cpp
// Fusion performance evaluation
class FusionPerformanceEvaluator
{
public:
    struct PerformanceMetrics {
        double accuracy_improvement;    // Improvement over single sensors
        double consistency_score;       // Consistency of fusion results
        double computational_efficiency; // Processing time efficiency
        double robustness_score;        // Performance under sensor failures
        double reliability_score;       // Overall reliability of fusion
    };

    PerformanceMetrics evaluate_fusion_performance(
        const std::vector<SensorTestData> & test_data)
    {
        PerformanceMetrics metrics;

        // Test accuracy improvement
        metrics.accuracy_improvement = test_accuracy_improvement(test_data);

        // Test consistency
        metrics.consistency_score = test_consistency(test_data);

        // Test computational efficiency
        metrics.computational_efficiency = test_computational_efficiency(test_data);

        // Test robustness to sensor failures
        metrics.robustness_score = test_robustness(test_data);

        // Calculate overall reliability
        metrics.reliability_score = calculate_reliability_score(metrics);

        return metrics;
    }

private:
    double test_accuracy_improvement(const std::vector<SensorTestData> & test_data)
    {
        double single_sensor_error = 0.0;
        double fusion_error = 0.0;

        for (const auto & test : test_data) {
            // Calculate error for each individual sensor
            for (const auto & sensor : test.sensors) {
                single_sensor_error += calculate_error(sensor, test.ground_truth);
            }

            // Calculate error for fusion result
            fusion_error += calculate_error(test.fusion_result, test.ground_truth);
        }

        // Average errors
        single_sensor_error /= (test_data.size() * test_data[0].sensors.size());
        fusion_error /= test_data.size();

        // Calculate improvement ratio
        return (single_sensor_error - fusion_error) / single_sensor_error;
    }

    double test_consistency(const std::vector<SensorTestData> & test_data)
    {
        // Test how consistent fusion results are across similar conditions
        double consistency_variance = 0.0;

        for (size_t i = 1; i < test_data.size(); ++i) {
            double diff = calculate_difference(
                test_data[i].fusion_result,
                test_data[i-1].fusion_result
            );

            consistency_variance += diff * diff;
        }

        consistency_variance /= (test_data.size() - 1);

        // Convert variance to consistency score (lower variance = higher consistency)
        return 1.0 / (1.0 + consistency_variance);
    }

    double test_computational_efficiency(const std::vector<SensorTestData> & test_data)
    {
        auto start_time = std::chrono::high_resolution_clock::now();

        for (const auto & test : test_data) {
            auto fusion_result = perform_fusion(test.sensors);
        }

        auto end_time = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(
            end_time - start_time
        );

        // Calculate efficiency based on processing time vs. required frequency
        double avg_processing_time = duration.count() / test_data.size();
        double required_frequency = 20.0; // 20 Hz for navigation

        return required_frequency * avg_processing_time < 1.0 ? 1.0 :
               1.0 / (required_frequency * avg_processing_time);
    }

    double test_robustness(const std::vector<SensorTestData> & test_data)
    {
        double robustness_score = 0.0;
        int test_count = 0;

        // Test fusion performance when individual sensors fail
        for (const auto & test : test_data) {
            for (size_t i = 0; i < test.sensors.size(); ++i) {
                // Create sensor data with one sensor "failed"
                auto degraded_data = test.sensors;
                degraded_data.erase(degraded_data.begin() + i);

                auto degraded_result = perform_fusion(degraded_data);
                double degraded_error = calculate_error(degraded_result, test.ground_truth);

                // Compare to full fusion result
                double full_error = calculate_error(test.fusion_result, test.ground_truth);

                // Robustness is how much performance degrades
                robustness_score += (full_error / (degraded_error + 0.001));
                test_count++;
            }
        }

        return robustness_score / test_count;
    }

    double calculate_reliability_score(const PerformanceMetrics & metrics)
    {
        // Weighted average of all metrics
        return 0.3 * metrics.accuracy_improvement +
               0.2 * metrics.consistency_score +
               0.2 * metrics.computational_efficiency +
               0.2 * metrics.robustness_score +
               0.1 * metrics.reliability_score;
    }

    double calculate_error(const SensorFusionResult & result,
                          const GroundTruth & truth)
    {
        // Calculate error between result and ground truth
        double dx = result.position.x - truth.position.x;
        double dy = result.position.y - truth.position.y;
        double dz = result.position.z - truth.position.z;

        return sqrt(dx*dx + dy*dy + dz*dz);
    }

    double calculate_difference(const SensorFusionResult & a,
                               const SensorFusionResult & b)
    {
        // Calculate difference between two fusion results
        double dx = a.position.x - b.position.x;
        double dy = a.position.y - b.position.y;

        return sqrt(dx*dx + dy*dy);
    }

    SensorFusionResult perform_fusion(const std::vector<SensorData> & sensors)
    {
        // Perform the actual fusion operation
        return SensorFusionResult(); // Simplified for example
    }
};
```

## Learning Objectives

After studying this section, students should be able to:

1. Understand different sensor modalities and their characteristics for humanoid navigation
2. Implement Kalman filter and particle filter-based sensor fusion
3. Design fusion strategies that consider humanoid-specific constraints
4. Integrate sensor fusion into the Nav2 framework
5. Evaluate and optimize fusion performance
6. Handle sensor failures and maintain navigation reliability

## References

1. Thrun, S., Burgard, W., & Fox, D. (2005). Probabilistic Robotics. MIT Press.
2. Bar-Shalom, Y., Li, X. R., & Kirubarajan, T. (2001). Estimation with Applications to Tracking and Navigation. John Wiley & Sons.
3. Fox, D., Burgard, W., & Thrun, S. (1999). Markov localization for mobile robots in dynamic environments. Journal of Artificial Intelligence Research, 11, 351-384.
4. Nav2 Documentation. (2023). ROS 2 Navigation Stack Sensor Fusion. https://navigation.ros.org/configuration/index.html
5. Englsberger, J., et al. (2011). Walking control of the humanoid robot ARMAR-III based on linear inverted pendulum tracking and geometric footstep placement. IEEE-RAS International Conference on Humanoid Robots.
6. Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. Journal of Basic Engineering, 82(1), 35-45.

---

This document was generated as part of the AI-Native Textbook on Physical AI & Humanoid Robotics. For more information, visit the project repository.