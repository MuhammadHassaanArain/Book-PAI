# 3.3 Depth Camera Simulation

## Introduction

Depth camera simulation is essential for robotics applications requiring 3D scene understanding, including navigation, manipulation, and human-robot interaction. This chapter covers the implementation of realistic depth camera sensors in Gazebo, including stereo vision principles, structured light simulation, noise modeling, and integration with ROS 2. Depth cameras provide rich 3D information that complements other sensors in robotic perception systems.

## Depth Camera Technologies

### Stereo Vision Cameras

Stereo vision systems use two cameras to triangulate depth:

```
Depth = (Baseline × Focal Length) / Disparity
```

Where:
- Baseline: Distance between camera centers
- Focal Length: Camera focal length in pixels
- Disparity: Difference in pixel positions between left and right images

### Time-of-Flight (ToF) Cameras

ToF cameras measure the time light takes to travel to objects and back:

```
Depth = (Speed of Light × Time of Flight) / 2
```

### Structured Light Cameras

Structured light systems project known patterns and analyze distortions:

- **Infrared Dot Projectors**: Like Microsoft Kinect
- **Line Projectors**: Multiple line patterns
- **Grid Projectors**: Grid pattern projection

## Gazebo Depth Camera Implementation

### SDF Configuration

Depth cameras in Gazebo are configured using the `depth_camera` sensor type:

```xml
<sensor name="depth_camera" type="depth">
  <pose>0.2 0 0.6 0 0 0</pose>  <!-- Position relative to parent link -->
  <visualize>true</visualize>    <!-- Show visualization in GUI -->
  <update_rate>30</update_rate>  <!-- Update rate in Hz -->

  <camera>
    <horizontal_fov>1.0472</horizontal_fov>  <!-- 60 degrees in radians -->
    <image>
      <width>640</width>
      <height>480</height>
      <format>R8G8B8</format>
    </image>
    <clip>
      <near>0.1</near>    <!-- Near clipping distance -->
      <far>10.0</far>     <!-- Far clipping distance -->
    </clip>
    <noise>
      <type>gaussian</type>
      <mean>0.0</mean>
      <stddev>0.007</stddev>
    </noise>
  </camera>

  <plugin name="camera_controller" filename="libgazebo_ros_openni_kinect.so">
    <ros>
      <namespace>robot1</namespace>
      <remapping>~/depth/image_raw:=depth/image_raw</remapping>
      <remapping>~/rgb/image_raw:=rgb/image_raw</remapping>
      <remapping>~/depth/camera_info:=depth/camera_info</remapping>
    </ros>
    <frame_name>robot1/depth_camera_optical_frame</frame_name>
    <baseline>0.1</baseline>
    <distortion_k1>0.0</distortion_k1>
    <distortion_k2>0.0</distortion_k2>
    <distortion_k3>0.0</distortion_k3>
    <distortion_t1>0.0</distortion_t1>
    <distortion_t2>0.0</distortion_t2>
    <point_cloud_cutoff>0.1</point_cloud_cutoff>
    <point_cloud_cutoff_max>3.0</point_cloud_cutoff_max>
    <Cx>320.0</Cx>
    <Cy>240.0</Cy>
    <focal_length>525.0</focal_length>
  </plugin>
</sensor>
```

### Advanced Depth Camera Configuration

For more sophisticated depth camera simulation:

```xml
<sensor name="advanced_depth_camera" type="depth">
  <pose>0.2 0 0.6 0 0 0</pose>
  <visualize>true</visualize>
  <update_rate>30</update_rate>

  <camera>
    <horizontal_fov>1.0472</horizontal_fov>
    <image>
      <width>1280</width>
      <height>720</height>
      <format>R8G8B8</format>
    </image>
    <clip>
      <near>0.05</near>
      <far>8.0</far>
    </clip>
    <noise>
      <type>gaussian</type>
      <mean>0.0</mean>
      <stddev>0.005</stddev>
    </noise>
  </camera>

  <plugin name="advanced_camera_controller" filename="libgazebo_ros_depth_camera.so">
    <ros>
      <namespace>robot1</namespace>
      <!-- Multiple output topics -->
      <remapping>~/depth/image_raw:=depth/image_raw</remapping>
      <remapping>~/rgb/image_raw:=rgb/image_raw</remapping>
      <remapping>~/depth/camera_info:=depth/camera_info</remapping>
      <remapping>~/points:=depth/points</remapping>
    </ros>
    <frame_name>robot1/depth_camera_optical_frame</frame_name>
    <baseline>0.075</baseline>
    <distortion_k1>0.0</distortion_k1>
    <distortion_k2>0.0</distortion_k2>
    <distortion_k3>0.0</distortion_k3>
    <distortion_t1>0.0</distortion_t1>
    <distortion_t2>0.0</distortion_t2>
    <point_cloud_cutoff>0.05</point_cloud_cutoff>
    <point_cloud_cutoff_max>8.0</point_cloud_cutoff_max>
    <Cx>640.0</Cx>
    <Cy>360.0</Cy>
    <focal_length>570.3422</focal_length>
    <hack_baseline>0.075</hack_baseline>
    <disable_placing_on_models>false</disable_placing_on_models>
  </plugin>
</sensor>
```

## Depth Image Generation and Processing

### Depth Image Format

Depth images store distance values in each pixel:

```python
import numpy as np
import cv2
from sensor_msgs.msg import Image
from cv_bridge import CvBridge

def process_depth_image(depth_data, width, height):
    """Process raw depth data into a depth image"""
    # Reshape the flat depth array into an image
    depth_image = np.array(depth_data).reshape((height, width))

    # Convert to appropriate format (meters)
    depth_image = depth_image.astype(np.float32)

    # Handle invalid depth values (usually 0 or inf)
    depth_image[depth_image <= 0] = np.inf
    depth_image[depth_image > 10.0] = np.inf  # Beyond sensor range

    return depth_image

def create_depth_ros_message(depth_image, header):
    """Create a ROS Image message from depth data"""
    bridge = CvBridge()

    # Convert to 32-bit float format
    depth_msg = bridge.cv2_to_imgmsg(depth_image, encoding='32FC1')
    depth_msg.header = header

    return depth_msg
```

### Point Cloud Generation from Depth

Convert depth images to 3D point clouds:

```python
def depth_to_pointcloud(depth_image, camera_info):
    """Convert depth image to point cloud"""
    height, width = depth_image.shape

    # Get camera intrinsic parameters
    fx = camera_info.k[0]  # Focal length x
    fy = camera_info.k[4]  # Focal length y
    cx = camera_info.k[2]  # Principal point x
    cy = camera_info.k[5]  # Principal point y

    points = []

    for v in range(height):
        for u in range(width):
            depth_val = depth_image[v, u]

            if np.isfinite(depth_val) and depth_val > 0:
                # Convert pixel coordinates to 3D world coordinates
                x = (u - cx) * depth_val / fx
                y = (v - cy) * depth_val / fy
                z = depth_val

                points.append([x, y, z])

    return np.array(points)

def create_pointcloud2_msg(points, header, frame_id):
    """Create PointCloud2 message from 3D points"""
    from sensor_msgs.msg import PointCloud2, PointField
    import sensor_msgs.point_cloud2 as pc2

    fields = [
        PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
        PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
        PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1)
    ]

    header.frame_id = frame_id
    return pc2.create_cloud(header, fields, points)
```

## Noise Modeling for Depth Cameras

### Depth-Specific Noise Models

Depth cameras have unique noise characteristics:

```python
class DepthCameraNoiseModel:
    def __init__(self, parameters):
        self.range_noise_factor = parameters.range_noise_factor
        self.baseline_noise = parameters.baseline_noise
        self.random_walk_noise = parameters.random_walk_noise
        self.bias = parameters.bias

    def add_noise(self, depth_image):
        """Add realistic noise to depth image"""
        noisy_depth = depth_image.copy()

        # Distance-dependent noise (quadratic model)
        distance_factor = 1.0 + self.range_noise_factor * depth_image
        noise_std = 0.001 * distance_factor  # Base noise increases with distance

        # Add Gaussian noise
        noise = np.random.normal(self.bias, noise_std, depth_image.shape)

        # Add noise to valid depth values only
        valid_mask = np.isfinite(depth_image) & (depth_image > 0)
        noisy_depth[valid_mask] = depth_image[valid_mask] + noise[valid_mask]

        # Ensure no negative depths
        noisy_depth[noisy_depth < 0] = 0

        return noisy_depth

    def simulate_sensor_limits(self, depth_image):
        """Simulate depth camera limitations"""
        # Near/far clipping
        depth_image = np.clip(depth_image, 0.1, 8.0)  # 10cm to 8m range

        # Edge effects (noisy edges)
        height, width = depth_image.shape
        edge_mask = np.zeros_like(depth_image, dtype=bool)
        edge_mask[0:10, :] = True  # Top edge
        edge_mask[-10:, :] = True  # Bottom edge
        edge_mask[:, 0:10] = True  # Left edge
        edge_mask[:, -10:] = True  # Right edge

        # Add extra noise to edges
        edge_noise = np.random.normal(0, 0.05, depth_image.shape)
        depth_image[edge_mask] += edge_noise[edge_mask]

        return depth_image
```

### Environmental Effects on Depth

Environmental conditions affect depth camera performance:

```python
class EnvironmentalDepthEffects:
    def __init__(self):
        self.near_infrared_attenuation = 0.05  # per meter
        self.ambient_light_noise = 0.01
        self.temporal_noise_factor = 0.005

    def apply_environmental_effects(self, depth_image, conditions):
        """Apply environmental effects to depth image"""
        result = depth_image.copy()

        # Lighting effects
        if conditions.ambient_light > 10000:  # Bright sunlight
            # Increase noise due to ambient light interference
            noise = np.random.normal(0, self.ambient_light_noise, depth_image.shape)
            result += noise

        # Weather effects
        if conditions.weather == 'rain':
            # Rain drops on lens, reduced accuracy
            rain_effect = np.random.uniform(0, 0.02, depth_image.shape)
            result += rain_effect
        elif conditions.weather == 'fog':
            # Fog reduces effective range
            visibility_factor = np.exp(-conditions.fog_density * depth_image)
            result = result * visibility_factor

        # Temperature effects on sensor
        if conditions.temperature > 40:  # High temperature
            thermal_noise = np.random.normal(0, self.temporal_noise_factor * 2,
                                           depth_image.shape)
            result += thermal_noise

        return result
```

## Stereo Vision Simulation

### Stereo Depth Estimation

Simulate stereo depth computation:

```python
class StereoDepthSimulator:
    def __init__(self, baseline, focal_length, width, height):
        self.baseline = baseline
        self.focal_length = focal_length
        self.width = width
        self.height = height

        # Precompute rectification maps (simplified)
        self.left_cam_matrix = np.array([
            [focal_length, 0, width/2],
            [0, focal_length, height/2],
            [0, 0, 1]
        ])

        self.right_cam_matrix = self.left_cam_matrix.copy()
        self.right_cam_matrix[0, 2] -= baseline  # Adjust for baseline

    def generate_stereo_pair(self, scene, pose):
        """Generate left and right camera images for stereo processing"""
        # Render scene from left camera pose
        left_image = self.render_scene(scene, pose, 'left')

        # Render scene from right camera pose (offset by baseline)
        right_pose = pose.copy()
        right_pose.position.x -= self.baseline  # Offset for stereo baseline
        right_image = self.render_scene(scene, right_pose, 'right')

        return left_image, right_image

    def compute_disparity(self, left_image, right_image):
        """Compute disparity map from stereo pair"""
        # Use OpenCV's stereo matcher (simplified)
        stereo = cv2.StereoSGBM_create(
            minDisparity=0,
            numDisparities=64,
            blockSize=11,
            P1=8 * 3 * 11**2,
            P2=32 * 3 * 11**2,
            disp12MaxDiff=1,
            uniquenessRatio=15,
            speckleWindowSize=0,
            speckleRange=2,
            preFilterCap=63,
            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY
        )

        # Compute disparity
        disparity = stereo.compute(left_image, right_image).astype(np.float32)

        # Convert to depth
        depth_map = (self.baseline * self.focal_length) / (disparity + 1e-6)

        # Apply minimum disparity threshold
        depth_map[disparity < 1.0] = 0  # Invalid disparities

        return depth_map
```

## ROS 2 Integration

### Depth Camera Publishers

Depth cameras typically publish multiple data streams:

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CameraInfo, PointCloud2
from cv_bridge import CvBridge
import numpy as np

class DepthCameraSimulator(Node):
    def __init__(self):
        super().__init__('depth_camera_simulator')

        # Publishers for depth camera data
        self.depth_publisher = self.create_publisher(
            Image,
            '/robot1/depth/image_raw',
            10
        )

        self.rgb_publisher = self.create_publisher(
            Image,
            '/robot1/rgb/image_raw',
            10
        )

        self.camera_info_publisher = self.create_publisher(
            CameraInfo,
            '/robot1/depth/camera_info',
            10
        )

        self.pointcloud_publisher = self.create_publisher(
            PointCloud2,
            '/robot1/depth/points',
            10
        )

        # Timer for publishing at camera rate
        self.timer = self.create_timer(1.0/30.0, self.publish_depth_data)  # 30Hz

        # Camera parameters
        self.width = 640
        self.height = 480
        self.focal_length = 525.0
        self.cx = 320.0
        self.cy = 240.0

        # Initialize CvBridge
        self.bridge = CvBridge()

        # Create camera info message
        self.camera_info_msg = self.create_camera_info()

    def create_camera_info(self):
        """Create CameraInfo message with intrinsic parameters"""
        from sensor_msgs.msg import CameraInfo

        camera_info = CameraInfo()
        camera_info.header.frame_id = 'robot1/depth_camera_optical_frame'

        # Image dimensions
        camera_info.height = self.height
        camera_info.width = self.width

        # Intrinsic parameters (3x3 matrix stored as 9-element array)
        camera_info.k = [
            self.focal_length, 0.0, self.cx,
            0.0, self.focal_length, self.cy,
            0.0, 0.0, 1.0
        ]

        # Distortion parameters (5 coefficients)
        camera_info.d = [0.0, 0.0, 0.0, 0.0, 0.0]

        # Projection matrix (3x4)
        camera_info.p = [
            self.focal_length, 0.0, self.cx, 0.0,
            0.0, self.focal_length, self.cy, 0.0,
            0.0, 0.0, 1.0, 0.0
        ]

        return camera_info

    def publish_depth_data(self):
        """Generate and publish depth camera data"""
        # Generate simulated depth data (this would come from Gazebo)
        depth_image = self.generate_depth_image()
        rgb_image = self.generate_rgb_image()

        # Add noise to depth
        noisy_depth = self.add_depth_noise(depth_image)

        # Create ROS messages
        depth_msg = self.bridge.cv2_to_imgmsg(noisy_depth, encoding='32FC1')
        depth_msg.header.stamp = self.get_clock().now().to_msg()
        depth_msg.header.frame_id = 'robot1/depth_camera_optical_frame'

        rgb_msg = self.bridge.cv2_to_imgmsg(rgb_image, encoding='rgb8')
        rgb_msg.header = depth_msg.header

        camera_info_msg = self.camera_info_msg
        camera_info_msg.header.stamp = depth_msg.header.stamp

        # Publish messages
        self.depth_publisher.publish(depth_msg)
        self.rgb_publisher.publish(rgb_msg)
        self.camera_info_publisher.publish(camera_info_msg)

        # Generate and publish point cloud
        pointcloud_msg = self.depth_to_pointcloud_msg(noisy_depth, depth_msg.header)
        if pointcloud_msg is not None:
            self.pointcloud_publisher.publish(pointcloud_msg)

    def generate_depth_image(self):
        """Generate simulated depth image"""
        # This would interface with Gazebo's rendering system
        # For simulation, create a synthetic depth image
        depth_image = np.ones((self.height, self.width), dtype=np.float32) * 5.0

        # Add some objects at different depths
        center_x, center_y = self.width // 2, self.height // 2
        cv2.circle(depth_image, (center_x, center_y), 50, 2.0, -1)  # Close object
        cv2.circle(depth_image, (center_x + 100, center_y), 30, 3.5, -1)  # Medium object
        cv2.circle(depth_image, (center_x - 100, center_y), 40, 1.5, -1)  # Close object

        return depth_image

    def generate_rgb_image(self):
        """Generate corresponding RGB image"""
        rgb_image = np.zeros((self.height, self.width, 3), dtype=np.uint8)

        # Create a simple RGB image matching the depth objects
        center_x, center_y = self.width // 2, self.height // 2
        cv2.circle(rgb_image, (center_x, center_y), 50, (255, 0, 0), -1)  # Blue circle
        cv2.circle(rgb_image, (center_x + 100, center_y), 30, (0, 255, 0), -1)  # Green circle
        cv2.circle(rgb_image, (center_x - 100, center_y), 40, (0, 0, 255), -1)  # Red circle

        return rgb_image

    def add_depth_noise(self, depth_image):
        """Add realistic noise to depth image"""
        # Add distance-dependent noise
        noise_std = 0.001 + 0.002 * depth_image  # Noise increases with distance
        noise = np.random.normal(0, noise_std, depth_image.shape)

        noisy_depth = depth_image + noise

        # Ensure valid depth values
        noisy_depth[noisy_depth < 0.1] = 0.0  # Invalid near values
        noisy_depth[noisy_depth > 8.0] = 0.0   # Invalid far values

        return noisy_depth

    def depth_to_pointcloud_msg(self, depth_image, header):
        """Convert depth image to PointCloud2 message"""
        height, width = depth_image.shape

        # Get camera intrinsic parameters
        fx_inv = 1.0 / self.focal_length
        fy_inv = 1.0 / self.focal_length
        cx = self.cx
        cy = self.cy

        points = []

        for v in range(height):
            for u in range(width):
                depth_val = depth_image[v, u]

                if depth_val > 0 and depth_val < 8.0:  # Valid depth range
                    # Convert pixel to 3D coordinates
                    x = (u - cx) * depth_val * fx_inv
                    y = (v - cy) * depth_val * fy_inv
                    z = depth_val

                    points.append([x, y, z])

        if len(points) == 0:
            return None

        # Create PointCloud2 message
        from sensor_msgs.msg import PointCloud2, PointField
        import sensor_msgs.point_cloud2 as pc2

        fields = [
            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1)
        ]

        return pc2.create_cloud(header, fields, points)
```

### TF Integration for Depth Cameras

Proper coordinate frame management for depth cameras:

```python
import tf2_ros
from geometry_msgs.msg import TransformStamped

class DepthCameraTFPublisher(Node):
    def __init__(self):
        super().__init__('depth_camera_tf_publisher')

        # Create transform broadcaster
        self.tf_broadcaster = tf2_ros.TransformBroadcaster(self)

        # Timer for publishing transforms
        self.tf_timer = self.create_timer(0.01, self.publish_transforms)  # 100Hz

    def publish_transforms(self):
        """Publish necessary transforms for depth camera"""
        # Base link to camera link
        t1 = TransformStamped()
        t1.header.stamp = self.get_clock().now().to_msg()
        t1.header.frame_id = 'robot1/base_link'
        t1.child_frame_id = 'robot1/depth_camera_link'

        t1.transform.translation.x = 0.2  # Forward
        t1.transform.translation.y = 0.0  # Left/right
        t1.transform.translation.z = 0.6  # Height

        t1.transform.rotation.x = 0.0
        t1.transform.rotation.y = 0.0
        t1.transform.rotation.z = 0.0
        t1.transform.rotation.w = 1.0

        self.tf_broadcaster.sendTransform(t1)

        # Camera link to optical frame (for proper camera conventions)
        t2 = TransformStamped()
        t2.header.stamp = self.get_clock().now().to_msg()
        t2.header.frame_id = 'robot1/depth_camera_link'
        t2.child_frame_id = 'robot1/depth_camera_optical_frame'

        # Optical frame: x-right, y-down, z-forward (camera convention)
        # This requires a coordinate system rotation from the camera link
        t2.transform.translation.x = 0.0
        t2.transform.translation.y = 0.0
        t2.transform.translation.z = 0.0

        # Rotation: 90° around Z then 90° around X
        # This transforms from robot frame to optical frame
        import math
        t2.transform.rotation.x = 0.5
        t2.transform.rotation.y = 0.5
        t2.transform.rotation.z = -0.5
        t2.transform.rotation.w = 0.5

        self.tf_broadcaster.sendTransform(t2)
```

## Performance Optimization

### Efficient Depth Processing

Optimize depth image processing for real-time performance:

```python
import numba
import numpy as np

@numba.jit(nopython=True)
def fast_depth_processing(depth_image, valid_range_min, valid_range_max):
    """Fast depth image processing using Numba JIT compilation"""
    height, width = depth_image.shape
    processed = np.zeros_like(depth_image)

    for i in range(height):
        for j in range(width):
            val = depth_image[i, j]
            if valid_range_min <= val <= valid_range_max:
                processed[i, j] = val
            else:
                processed[i, j] = 0.0  # Invalid value

    return processed

class OptimizedDepthProcessor:
    def __init__(self):
        self.cache_size = 10
        self.processed_frames = {}

    def process_depth_with_cache(self, depth_image, timestamp):
        """Process depth image with caching for repeated operations"""
        # Simple cache based on timestamp
        if timestamp in self.processed_frames:
            return self.processed_frames[timestamp]

        processed = fast_depth_processing(depth_image, 0.1, 8.0)

        # Cache recent frames
        if len(self.processed_frames) >= self.cache_size:
            # Remove oldest entry
            oldest_key = min(self.processed_frames.keys())
            del self.processed_frames[oldest_key]

        self.processed_frames[timestamp] = processed
        return processed
```

### Multi-Resolution Processing

Use different processing levels based on application needs:

```python
def multi_resolution_depth_processing(depth_image, quality_level='high'):
    """Process depth image at different resolutions based on quality setting"""

    if quality_level == 'low':
        # Fast processing with reduced resolution
        small_depth = cv2.resize(depth_image, (depth_image.shape[1]//4,
                                             depth_image.shape[0]//4))
        # Process at lower resolution
        processed_small = fast_depth_processing(small_depth, 0.1, 8.0)
        # Upsample back to original size
        result = cv2.resize(processed_small, (depth_image.shape[1],
                                            depth_image.shape[0]),
                           interpolation=cv2.INTER_NEAREST)

    elif quality_level == 'medium':
        # Medium resolution processing
        small_depth = cv2.resize(depth_image, (depth_image.shape[1]//2,
                                             depth_image.shape[0]//2))
        processed_small = fast_depth_processing(small_depth, 0.1, 8.0)
        result = cv2.resize(processed_small, (depth_image.shape[1],
                                           depth_image.shape[0]),
                           interpolation=cv2.INTER_LINEAR)

    else:  # high quality
        result = fast_depth_processing(depth_image, 0.1, 8.0)

    return result
```

## Validation and Testing

### Depth Accuracy Validation

```python
def validate_depth_camera_accuracy(simulated_depth, ground_truth_depth, tolerance=0.05):
    """Validate simulated depth camera against ground truth"""
    # Calculate absolute errors
    valid_mask = (ground_truth_depth > 0) & (ground_truth_depth < 10.0)
    errors = np.abs(simulated_depth[valid_mask] - ground_truth_depth[valid_mask])

    # Statistics
    mean_error = np.mean(errors)
    std_error = np.std(errors)
    max_error = np.max(errors)
    rmse = np.sqrt(np.mean(errors**2))

    # Success rate within tolerance
    success_rate = np.sum(errors < tolerance) / len(errors) * 100

    print(f"Depth Camera Validation Results:")
    print(f"  Mean Error: {mean_error:.3f} m")
    print(f"  Std Dev: {std_error:.3f} m")
    print(f"  Max Error: {max_error:.3f} m")
    print(f"  RMSE: {rmse:.3f} m")
    print(f"  Success Rate (<{tolerance}m): {success_rate:.1f}%")

    return {
        'mean_error': mean_error,
        'std_error': std_error,
        'max_error': max_error,
        'rmse': rmse,
        'success_rate': success_rate,
        'pass': mean_error < 0.02  # Pass if mean error < 2cm
    }
```

### Point Cloud Quality Assessment

```python
def assess_pointcloud_quality(simulated_pc, ground_truth_pc, metrics=['density', 'completeness', 'accuracy']):
    """Assess the quality of depth camera-generated point clouds"""
    import open3d as o3d

    # Convert to Open3D point clouds
    sim_pcd = o3d.geometry.PointCloud()
    sim_pcd.points = o3d.utility.Vector3dVector(simulated_pc)

    gt_pcd = o3d.geometry.PointCloud()
    gt_pcd.points = o3d.utility.Vector3dVector(ground_truth_pc)

    results = {}

    if 'density' in metrics:
        # Calculate point density
        sim_density = calculate_point_density(sim_pcd)
        gt_density = calculate_point_density(gt_pcd)
        results['density_ratio'] = sim_density / gt_density

    if 'completeness' in metrics:
        # How much of the ground truth is captured
        distances = sim_pcd.compute_point_cloud_distance(gt_pcd)
        completeness = len([d for d in distances if d < 0.02]) / len(distances)  # Within 2cm
        results['completeness'] = completeness

    if 'accuracy' in metrics:
        # How accurate are the simulated points
        distances = gt_pcd.compute_point_cloud_distance(sim_pcd)
        accuracy = np.mean(distances)
        results['accuracy'] = accuracy

    return results
```

## Common Issues and Troubleshooting

### Performance Issues

- **Slow Processing**: Use optimized libraries (Numba, OpenCV) and consider lower resolution
- **Memory Usage**: Process frames in batches and clear unused data
- **CPU Load**: Use multi-threading for intensive operations

### Accuracy Issues

- **Wrong Scale**: Verify units are in meters, not millimeters
- **Inverted Axes**: Check coordinate frame conventions
- **Noise Too High**: Adjust noise parameters based on real sensor specifications

### Integration Issues

- **TF Problems**: Ensure optical frame conventions are followed
- **Synchronization**: Depth and RGB images should be timestamped together
- **Calibration**: Verify camera intrinsic parameters are correct

## Best Practices

### 1. Follow Optical Frame Conventions

Use the standard optical frame convention: X-right, Y-down, Z-forward relative to the camera.

### 2. Include Realistic Noise Models

Model depth-specific noise characteristics including distance-dependent noise.

### 3. Validate Against Real Data

Compare simulated depth outputs to real depth camera data when available.

### 4. Optimize for Performance

Balance depth accuracy with computational requirements, especially for real-time applications.

### 5. Proper TF Management

Maintain correct coordinate frame relationships for proper depth processing.

## Summary

Depth camera simulation in Gazebo provides essential 3D perception capabilities for robotics applications. By understanding the principles of depth sensing, implementing proper noise models, and optimizing for performance, you can create realistic depth camera simulations suitable for navigation, manipulation, and scene understanding tasks. The integration with ROS 2 enables seamless use of simulated depth data in perception pipelines.

## References

1. Open Source Robotics Foundation. (2023). Gazebo Depth Camera Tutorial. https://gazebosim.org/tutorials?tut=ros_gzplugins_sensors
2. Zhang, Z. (2000). A Flexible New Technique for Camera Calibration. IEEE Transactions on Pattern Analysis and Machine Intelligence.
3. Newcombe, R.A., et al. (2011). KinectFusion: Real-time Dense Surface Mapping and Tracking. ISMAR.

## Exercises

1. Implement a stereo vision depth camera simulation with realistic noise
2. Create a validation pipeline comparing simulated and real depth data
3. Develop performance optimization for real-time depth processing
4. Design a structured light depth camera simulation model