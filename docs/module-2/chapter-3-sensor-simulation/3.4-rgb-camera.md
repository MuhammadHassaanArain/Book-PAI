# 3.4 RGB Camera & Optical Models

## Introduction

RGB camera simulation is fundamental for computer vision applications in robotics, including object recognition, visual SLAM, and human-robot interaction. This chapter covers the implementation of realistic RGB cameras in Gazebo, including optical models, image formation, distortion effects, and integration with ROS 2. RGB cameras provide rich visual information that enables advanced perception capabilities in robotic systems.

## Camera Optics and Image Formation

### Pinhole Camera Model

The pinhole camera model is the foundation for most camera simulations:

```
x = f * X / Z
y = f * Y / Z
```

Where:
- (x, y) are image coordinates
- (X, Y, Z) are 3D world coordinates
- f is the focal length

### Camera Intrinsic Parameters

The camera intrinsic matrix combines focal lengths and principal point:

```
K = [fx  0  cx]
    [0  fy  cy]
    [0  0   1]
```

Where:
- fx, fy: Focal lengths in pixels (may differ due to rectangular pixels)
- cx, cy: Principal point (optical center) in pixels

### Lens Distortion Models

Real cameras have lens distortions that must be modeled:

#### Radial Distortion
```
x_corrected = x * (1 + k1*r² + k2*r⁴ + k3*r⁶)
y_corrected = y * (1 + k1*r² + k2*r⁴ + k3*r⁶)
```

Where r² = x² + y² and k1, k2, k3 are radial distortion coefficients.

#### Tangential Distortion
```
x_corrected = x + [2*p1*x*y + p2*(r² + 2*x²)]
y_corrected = y + [p1*(r² + 2*y²) + 2*p2*x*y]
```

Where p1, p2 are tangential distortion coefficients.

## Gazebo RGB Camera Implementation

### SDF Configuration

RGB cameras in Gazebo are configured using the `camera` sensor type:

```xml
<sensor name="rgb_camera" type="camera">
  <pose>0.2 0 0.6 0 0 0</pose>  <!-- Position relative to parent link -->
  <visualize>true</visualize>    <!-- Show camera visualization in GUI -->
  <update_rate>30</update_rate>  <!-- Update rate in Hz -->

  <camera>
    <horizontal_fov>1.0472</horizontal_fov>  <!-- 60 degrees in radians -->
    <image>
      <width>640</width>
      <height>480</height>
      <format>R8G8B8</format>
    </image>
    <clip>
      <near>0.1</near>    <!-- Near clipping distance -->
      <far>300.0</far>    <!-- Far clipping distance -->
    </clip>
    <distortion>
      <k1>0.0</k1>        <!-- Radial distortion coefficient 1 -->
      <k2>0.0</k2>        <!-- Radial distortion coefficient 2 -->
      <k3>0.0</k3>        <!-- Radial distortion coefficient 3 -->
      <p1>0.0</p1>        <!-- Tangential distortion coefficient 1 -->
      <p2>0.0</p2>        <!-- Tangential distortion coefficient 2 -->
      <center>0.5 0.5</center>  <!-- Principal point (normalized) -->
    </distortion>
  </camera>

  <plugin name="camera_controller" filename="libgazebo_ros_camera.so">
    <ros>
      <namespace>robot1</namespace>
      <remapping>~/image_raw:=rgb/image_raw</remapping>
      <remapping>~/camera_info:=rgb/camera_info</remapping>
    </ros>
    <frame_name>robot1/rgb_camera_optical_frame</frame_name>
    <min_distance>0.1</min_distance>
    <max_distance>300.0</max_distance>
    <hack_baseline>0.075</hack_baseline>
  </plugin>
</sensor>
```

### High-Resolution Camera Configuration

For high-quality RGB camera simulation:

```xml
<sensor name="high_res_camera" type="camera">
  <pose>0.2 0 0.6 0 0 0</pose>
  <visualize>false</visualize>  <!-- Disable visualization for performance -->
  <update_rate>15</update_rate>  <!-- Lower rate for high resolution -->

  <camera>
    <horizontal_fov>0.7854</horizontal_fov>  <!-- 45 degrees (narrower FOV for detail) -->
    <image>
      <width>1920</width>    <!-- Full HD resolution -->
      <height>1080</height>
      <format>R8G8B8</format>
    </image>
    <clip>
      <near>0.05</near>
      <far>500.0</far>
    </clip>
    <distortion>
      <k1>-0.28340811</k1>
      <k2>0.07395907</k2>
      <k3>0.00019359</k3>
      <p1>0.00019359</p1>
      <p2>0.00014516</p2>
      <center>0.5 0.5</center>
    </distortion>
  </camera>

  <plugin name="high_res_camera_controller" filename="libgazebo_ros_camera.so">
    <ros>
      <namespace>robot1</namespace>
      <remapping>~/image_raw:=high_res/image_raw</remapping>
      <remapping>~/camera_info:=high_res/camera_info</remapping>
    </ros>
    <frame_name>robot1/rgb_camera_optical_frame</frame_name>
    <min_distance>0.05</min_distance>
    <max_distance>500.0</max_distance>
    <hack_baseline>0.0</hack_baseline>
  </plugin>
</sensor>
```

### Multiple Camera Setup

Configuring multiple cameras for stereo vision or multi-view systems:

```xml
<!-- Left camera -->
<sensor name="stereo_left" type="camera">
  <pose>0.2 -0.05 0.6 0 0 0</pose>
  <update_rate>30</update_rate>
  <camera>
    <horizontal_fov>1.0472</horizontal_fov>
    <image><width>640</width><height>480</height><format>R8G8B8</format></image>
    <clip><near>0.1</near><far>10.0</far></clip>
  </camera>
  <plugin name="left_camera" filename="libgazebo_ros_camera.so">
    <ros>
      <namespace>robot1</namespace>
      <remapping>~/image_raw:=stereo/left/image_raw</remapping>
      <remapping>~/camera_info:=stereo/left/camera_info</remapping>
    </ros>
    <frame_name>robot1/stereo_left_optical_frame</frame_name>
  </plugin>
</sensor>

<!-- Right camera -->
<sensor name="stereo_right" type="camera">
  <pose>0.2 0.05 0.6 0 0 0</pose>
  <update_rate>30</update_rate>
  <camera>
    <horizontal_fov>1.0472</horizontal_fov>
    <image><width>640</width><height>480</height><format>R8G8B8</format></image>
    <clip><near>0.1</near><far>10.0</far></clip>
  </camera>
  <plugin name="right_camera" filename="libgazebo_ros_camera.so">
    <ros>
      <namespace>robot1</namespace>
      <remapping>~/image_raw:=stereo/right/image_raw</remapping>
      <remapping>~/camera_info:=stereo/right/camera_info</remapping>
    </ros>
    <frame_name>robot1/stereo_right_optical_frame</frame_name>
  </plugin>
</sensor>
```

## Image Processing and Distortion

### Camera Calibration Parameters

Camera calibration involves determining intrinsic and extrinsic parameters:

```python
import numpy as np
import cv2

class CameraCalibrator:
    def __init__(self):
        self.camera_matrix = None
        self.dist_coeffs = None
        self.rvecs = None
        self.tvecs = None

    def calibrate_camera(self, object_points, image_points, image_size):
        """Calibrate camera using chessboard pattern"""
        # Object points are 3D points in real world space (z=0 for planar pattern)
        # Image points are 2D points in image space

        # Camera matrix (intrinsic parameters)
        camera_matrix = np.eye(3)
        camera_matrix[0, 0] = 525.0  # fx
        camera_matrix[1, 1] = 525.0  # fy
        camera_matrix[0, 2] = image_size[0] / 2  # cx
        camera_matrix[1, 2] = image_size[1] / 2  # cy

        # Distortion coefficients
        dist_coeffs = np.zeros((5, 1))  # [k1, k2, p1, p2, k3]

        # Calibrate camera
        ret, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(
            object_points, image_points, image_size, camera_matrix, dist_coeffs
        )

        self.camera_matrix = camera_matrix
        self.dist_coeffs = dist_coeffs
        self.rvecs = rvecs
        self.tvecs = tvecs

        return ret, camera_matrix, dist_coeffs

    def undistort_image(self, image):
        """Remove distortion from image using calibration parameters"""
        if self.camera_matrix is None or self.dist_coeffs is None:
            return image

        h, w = image.shape[:2]
        new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(
            self.camera_matrix, self.dist_coeffs, (w, h), 1, (w, h)
        )

        undistorted = cv2.undistort(
            image, self.camera_matrix, self.dist_coeffs,
            None, new_camera_matrix
        )

        # Crop the image based on ROI
        x, y, w, h = roi
        undistorted = undistorted[y:y+h, x:x+w]

        return undistorted
```

### Synthetic Distortion Generation

Simulate realistic camera distortions:

```python
class SyntheticDistortion:
    def __init__(self, camera_matrix, dist_coeffs):
        self.camera_matrix = camera_matrix
        self.dist_coeffs = dist_coeffs

    def apply_distortion(self, image):
        """Apply distortion to a synthetic image"""
        h, w = image.shape[:2]

        # Generate coordinate grids
        x, y = np.meshgrid(np.arange(w), np.arange(h))
        x = x.astype(np.float32)
        y = y.astype(np.float32)

        # Convert to normalized coordinates
        x_norm = (x - self.camera_matrix[0, 2]) / self.camera_matrix[0, 0]
        y_norm = (y - self.camera_matrix[1, 2]) / self.camera_matrix[1, 1]

        # Apply distortion
        r2 = x_norm**2 + y_norm**2
        r4 = r2**2
        r6 = r2**3

        k1, k2, p1, p2, k3 = self.dist_coeffs.flatten()

        # Radial distortion
        radial_factor = 1 + k1*r2 + k2*r4 + k3*r6

        # Tangential distortion
        dx = 2*p1*x_norm*y_norm + p2*(r2 + 2*x_norm**2)
        dy = p1*(r2 + 2*y_norm**2) + 2*p2*x_norm*y_norm

        # Apply distortions
        x_distorted = x_norm * radial_factor + dx
        y_distorted = y_norm * radial_factor + dy

        # Convert back to pixel coordinates
        x_distorted = x_distorted * self.camera_matrix[0, 0] + self.camera_matrix[0, 2]
        y_distorted = y_distorted * self.camera_matrix[1, 1] + self.camera_matrix[1, 2]

        # Create mapping for remapping
        map_x = x_distorted.astype(np.float32)
        map_y = y_distorted.astype(np.float32)

        # Apply remapping
        distorted_image = cv2.remap(image, map_x, map_y, cv2.INTER_LINEAR)

        return distorted_image
```

## Optical Effects Simulation

### Exposure and Dynamic Range

Simulate camera exposure effects:

```python
class ExposureSimulator:
    def __init__(self):
        self.exposure_time = 0.033  # 30 FPS default
        self.iso = 100
        self.aperture = 2.8

    def simulate_exposure(self, image, lighting_condition):
        """Simulate exposure based on lighting conditions"""
        # Convert to float for processing
        img_float = image.astype(np.float32) / 255.0

        # Apply exposure compensation based on lighting
        if lighting_condition == 'bright':
            exposure_factor = 0.7  # Reduce exposure in bright conditions
        elif lighting_condition == 'dim':
            exposure_factor = 1.5  # Increase exposure in dim conditions
        else:  # normal lighting
            exposure_factor = 1.0

        # Apply exposure
        exposed = img_float * exposure_factor

        # Add shot noise (photon noise)
        shot_noise = np.random.poisson(exposed * 255) / 255.0
        shot_noise = shot_noise / np.sqrt(255)  # Normalize

        # Add read noise
        read_noise = np.random.normal(0, 0.01, exposed.shape)

        # Combine effects
        result = exposed + shot_noise + read_noise

        # Clamp to valid range
        result = np.clip(result, 0, 1)

        # Convert back to uint8
        return (result * 255).astype(np.uint8)

    def simulate_motion_blur(self, image, motion_vector):
        """Simulate motion blur based on camera/object motion"""
        if np.linalg.norm(motion_vector) < 0.1:
            return image

        # Create motion blur kernel
        length = int(np.linalg.norm(motion_vector))
        angle = np.arctan2(motion_vector[1], motion_vector[0])

        # Create kernel
        kernel = np.zeros((length, length))
        kernel[int(length/2), :] = 1.0 / length

        # Rotate kernel
        rotation_matrix = np.array([
            [np.cos(angle), -np.sin(angle)],
            [np.sin(angle), np.cos(angle)]
        ])

        # Apply blur
        blurred = cv2.filter2D(image, -1, kernel)

        return blurred
```

### Color and White Balance

Simulate color effects and white balance:

```python
class ColorSimulator:
    def __init__(self):
        self.temperature = 6500  # Default daylight temperature in Kelvin
        self.tint = 0  # Green-magenta balance

    def simulate_color_temperature(self, image):
        """Simulate different color temperatures"""
        # Convert to float
        img_float = image.astype(np.float32) / 255.0

        # Color temperature to RGB gain
        if self.temperature < 6500:  # Warmer (more red)
            gains = np.array([1.2, 1.0, 0.8])  # More red, less blue
        elif self.temperature > 6500:  # Cooler (more blue)
            gains = np.array([0.8, 1.0, 1.2])  # Less red, more blue
        else:  # Neutral
            gains = np.array([1.0, 1.0, 1.0])

        # Apply gains
        img_float = img_float * gains[None, None, :]

        # Apply tint
        if self.tint > 0:  # More green
            img_float[:, :, 1] = np.clip(img_float[:, :, 1] + self.tint * 0.1, 0, 1)
        elif self.tint < 0:  # More magenta
            img_float[:, :, 0] = np.clip(img_float[:, :, 0] - self.tint * 0.1, 0, 1)
            img_float[:, :, 2] = np.clip(img_float[:, :, 2] - self.tint * 0.1, 0, 1)

        # Clamp and convert back
        img_float = np.clip(img_float, 0, 1)
        return (img_float * 255).astype(np.uint8)

    def simulate_gamma_correction(self, image):
        """Apply gamma correction"""
        # Convert to float [0, 1]
        img_float = image.astype(np.float32) / 255.0

        # Apply gamma correction (typical gamma = 2.2)
        gamma = 2.2
        corrected = np.power(img_float, 1.0/gamma)

        # Convert back to uint8
        return (corrected * 255).astype(np.uint8)
```

## Environmental Effects

### Atmospheric Effects

Simulate atmospheric conditions affecting camera performance:

```python
class AtmosphericEffects:
    def __init__(self):
        self.haze_density = 0.0
        self.fog_density = 0.0
        self.rain_density = 0.0

    def apply_atmospheric_effects(self, image, distance_map=None):
        """Apply atmospheric effects to image"""
        result = image.astype(np.float32) / 255.0

        # Haze effect (distance-based)
        if self.haze_density > 0 and distance_map is not None:
            haze_factor = 1.0 - np.exp(-self.haze_density * distance_map)
            haze_color = np.array([0.8, 0.85, 0.9])  # Light blue haze
            result = result * (1 - haze_factor[:, :, None]) + haze_color * haze_factor[:, :, None]

        # Fog effect (overall reduction in contrast)
        if self.fog_density > 0:
            fog_factor = self.fog_density * 0.1
            # Reduce contrast and add fog color
            fog_color = np.array([0.7, 0.75, 0.8])
            result = result * (1 - fog_factor) + fog_color * fog_factor

        # Rain effect (on lens)
        if self.rain_density > 0:
            # Add rain drops as dark spots
            rain_mask = np.random.random(result.shape[:2]) < self.rain_density * 0.05
            result[rain_mask] = result[rain_mask] * 0.7  # Darken rain spots

            # Add overall blur for rain effect
            kernel_size = int(self.rain_density * 3) + 1
            if kernel_size > 1:
                kernel = np.ones((kernel_size, kernel_size)) / (kernel_size**2)
                result = cv2.filter2D(result, -1, kernel)

        # Convert back to uint8
        result = np.clip(result, 0, 1)
        return (result * 255).astype(np.uint8)
```

### Lighting Effects

Simulate various lighting conditions:

```python
class LightingEffects:
    def __init__(self):
        self.ambient_light = 0.3
        self.directional_light = {'direction': [0, 0, -1], 'intensity': 1.0}
        self.shadow_softness = 0.5

    def simulate_lighting(self, image, normal_map=None):
        """Simulate lighting effects on image"""
        img_float = image.astype(np.float32) / 255.0

        # Ambient light (overall illumination)
        ambient = self.ambient_light
        result = img_float * ambient

        # Directional lighting (if normal map provided)
        if normal_map is not None:
            light_dir = np.array(self.directional_light['direction'])
            light_dir = light_dir / np.linalg.norm(light_dir)

            # Calculate dot product of normals and light direction
            # This is a simplified version - in practice, normal_map would be more complex
            lighting = np.dot(normal_map, light_dir)
            lighting = np.clip(lighting, 0, 1)  # Only positive lighting
            lighting = lighting * self.directional_light['intensity']

            result = result + img_float * lighting[:, :, None]

        # Add specular highlights
        # This would require more complex calculations in practice
        specular = np.random.random(img_float.shape[:2]) * 0.1 * (img_float.mean(axis=2) > 0.5)
        result = result + specular[:, :, None] * np.array([1.0, 1.0, 1.0])

        # Clamp and convert back
        result = np.clip(result, 0, 1)
        return (result * 255).astype(np.uint8)

    def simulate_lens_flare(self, image, light_source_pos):
        """Simulate lens flare effect"""
        h, w = image.shape[:2]
        result = image.astype(np.float32) / 255.0

        if light_source_pos is not None:
            # Calculate vector from image center to light source
            center = np.array([w/2, h/2])
            light_vec = np.array(light_source_pos) - center
            light_dist = np.linalg.norm(light_vec)

            if light_dist > 0:
                light_vec = light_vec / light_dist  # Normalize

                # Create flare effect
                for scale in [0.1, 0.05, 0.02]:  # Multiple flare elements
                    flare_pos = center + light_vec * light_dist * scale
                    x, y = int(flare_pos[0]), int(flare_pos[1])

                    if 0 <= x < w and 0 <= y < h:
                        # Add bright flare spot
                        size = max(1, int(light_dist * scale))
                        cv2.circle(result, (x, y), size, (1, 1, 1), -1)

        # Clamp and convert back
        result = np.clip(result, 0, 1)
        return (result * 255).astype(np.uint8)
```

## ROS 2 Integration

### RGB Camera Publishers

RGB cameras publish image and camera info messages:

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CameraInfo
from cv_bridge import CvBridge
import numpy as np
import cv2

class RGBCameraSimulator(Node):
    def __init__(self):
        super().__init__('rgb_camera_simulator')

        # Publishers for RGB camera data
        self.image_publisher = self.create_publisher(
            Image,
            '/robot1/rgb/image_raw',
            10
        )

        self.camera_info_publisher = self.create_publisher(
            CameraInfo,
            '/robot1/rgb/camera_info',
            10
        )

        # Timer for publishing at camera rate
        self.timer = self.create_timer(1.0/30.0, self.publish_camera_data)  # 30Hz

        # Camera parameters
        self.width = 640
        self.height = 480
        self.focal_length = 525.0
        self.cx = 320.0
        self.cy = 240.0

        # Initialize CvBridge
        self.bridge = CvBridge()

        # Create camera info message
        self.camera_info_msg = self.create_camera_info()

        # Initialize optical effects simulators
        self.exposure_sim = ExposureSimulator()
        self.color_sim = ColorSimulator()
        self.atmospheric_effects = AtmosphericEffects()

    def create_camera_info(self):
        """Create CameraInfo message with intrinsic parameters"""
        from sensor_msgs.msg import CameraInfo

        camera_info = CameraInfo()
        camera_info.header.frame_id = 'robot1/rgb_camera_optical_frame'

        # Image dimensions
        camera_info.height = self.height
        camera_info.width = self.width

        # Intrinsic parameters (3x3 matrix stored as 9-element array)
        camera_info.k = [
            self.focal_length, 0.0, self.cx,
            0.0, self.focal_length, self.cy,
            0.0, 0.0, 1.0
        ]

        # Distortion parameters (5 coefficients)
        camera_info.d = [0.0, 0.0, 0.0, 0.0, 0.0]

        # Identity rectification matrix (for monocular camera)
        camera_info.r = [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]

        # Projection matrix (3x4) - same as K for monocular
        camera_info.p = [
            self.focal_length, 0.0, self.cx, 0.0,
            0.0, self.focal_length, self.cy, 0.0,
            0.0, 0.0, 1.0, 0.0
        ]

        return camera_info

    def publish_camera_data(self):
        """Generate and publish RGB camera data"""
        # Generate simulated image (this would come from Gazebo rendering)
        image = self.generate_simulated_image()

        # Apply optical effects
        image = self.exposure_sim.simulate_exposure(image, 'normal')
        image = self.color_sim.simulate_color_temperature(image)
        image = self.color_sim.simulate_gamma_correction(image)

        # Create ROS Image message
        image_msg = self.bridge.cv2_to_imgmsg(image, encoding='rgb8')
        image_msg.header.stamp = self.get_clock().now().to_msg()
        image_msg.header.frame_id = 'robot1/rgb_camera_optical_frame'

        # Update camera info timestamp
        self.camera_info_msg.header.stamp = image_msg.header.stamp

        # Publish messages
        self.image_publisher.publish(image_msg)
        self.camera_info_publisher.publish(self.camera_info_msg)

    def generate_simulated_image(self):
        """Generate a simulated RGB image"""
        # This would interface with Gazebo's rendering system
        # For simulation, create a synthetic test image
        image = np.zeros((self.height, self.width, 3), dtype=np.uint8)

        # Create a test pattern with various colors and shapes
        # Center circle
        cv2.circle(image, (320, 240), 100, (255, 0, 0), -1)  # Blue circle

        # Upper rectangle
        cv2.rectangle(image, (200, 100), (440, 200), (0, 255, 0), -1)  # Green rectangle

        # Lower triangle
        pts = np.array([[320, 400], [250, 350], [390, 350]], np.int32)
        cv2.fillPoly(image, [pts], (0, 0, 255))  # Red triangle

        # Add some noise for realism
        noise = np.random.normal(0, 5, image.shape).astype(np.int16)
        image = np.clip(image.astype(np.int16) + noise, 0, 255).astype(np.uint8)

        return image
```

### Camera Calibration Integration

Integrate with ROS 2 camera calibration:

```python
class CalibrationPublisher(Node):
    def __init__(self):
        super().__init__('calibration_publisher')

        # Publisher for calibration data
        self.calibration_publisher = self.create_publisher(
            CameraInfo,
            '/robot1/rgb/camera_info',
            10
        )

        # Timer to publish calibration data
        self.timer = self.create_timer(1.0, self.publish_calibration)

        # Load calibration from file or parameters
        self.load_calibration()

    def load_calibration(self):
        """Load camera calibration from file or parameters"""
        # Example calibration parameters
        self.camera_matrix = np.array([
            [525.0, 0.0, 320.0],
            [0.0, 525.0, 240.0],
            [0.0, 0.0, 1.0]
        ])

        self.distortion_coeffs = np.array([0.0, 0.0, 0.0, 0.0, 0.0])

    def create_calibrated_camera_info(self):
        """Create CameraInfo message with calibration data"""
        from sensor_msgs.msg import CameraInfo

        camera_info = CameraInfo()
        camera_info.header.frame_id = 'robot1/rgb_camera_optical_frame'
        camera_info.height = 480
        camera_info.width = 640

        # Set intrinsic matrix
        camera_info.k = self.camera_matrix.flatten().tolist()
        camera_info.d = self.distortion_coeffs.flatten().tolist()
        camera_info.r = [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]
        camera_info.p = [
            self.camera_matrix[0, 0], 0.0, self.camera_matrix[0, 2], 0.0,
            0.0, self.camera_matrix[1, 1], self.camera_matrix[1, 2], 0.0,
            0.0, 0.0, 1.0, 0.0
        ]

        return camera_info
```

## Performance Optimization

### Efficient Image Processing

Optimize camera simulation for real-time performance:

```python
import numba
import numpy as np

@numba.jit(nopython=True)
def fast_color_correction(image, gains):
    """Fast color correction using Numba JIT"""
    height, width, channels = image.shape
    result = np.zeros_like(image, dtype=np.float32)

    for i in range(height):
        for j in range(width):
            for c in range(channels):
                result[i, j, c] = image[i, j, c] * gains[c]

    return np.clip(result, 0, 255).astype(np.uint8)

@numba.jit(nopython=True)
def fast_gamma_correction(image, gamma):
    """Fast gamma correction"""
    height, width, channels = image.shape
    result = np.zeros_like(image, dtype=np.float32)

    gamma_inv = 1.0 / gamma
    for i in range(height):
        for j in range(width):
            for c in range(channels):
                result[i, j, c] = 255 * ((image[i, j, c] / 255.0) ** gamma_inv)

    return result.astype(np.uint8)

class OptimizedCameraProcessor:
    def __init__(self):
        self.gamma = 2.2
        self.color_gains = np.array([1.0, 1.0, 1.0])

    def process_frame(self, image):
        """Process camera frame with optimized functions"""
        # Apply color correction
        corrected = fast_color_correction(image, self.color_gains)

        # Apply gamma correction
        final = fast_gamma_correction(corrected, self.gamma)

        return final
```

### Multi-Resolution Processing

Use different processing levels based on requirements:

```python
def adaptive_camera_processing(image, processing_level='high'):
    """Process camera image at different quality levels"""

    if processing_level == 'low':
        # Fast processing: skip complex effects
        return image

    elif processing_level == 'medium':
        # Apply basic effects only
        # Convert to float
        img_float = image.astype(np.float32) / 255.0

        # Basic exposure adjustment
        img_float = np.clip(img_float * 1.1, 0, 1)

        # Basic gamma correction
        img_float = np.power(np.clip(img_float, 0, 1), 1.0/2.2)

        return (img_float * 255).astype(np.uint8)

    else:  # high quality
        # Apply all effects
        # This would include full optical simulation
        return image  # Placeholder for full processing
```

## Validation and Testing

### Camera Accuracy Validation

```python
def validate_camera_calibration(simulated_points, real_points, camera_matrix, dist_coeffs):
    """Validate camera simulation against real calibration"""
    # Reproject 3D points to 2D using camera parameters
    projected_points, _ = cv2.projectPoints(
        real_points,
        np.zeros(3),  # Rotation vector (no rotation)
        np.zeros(3),  # Translation vector (no translation)
        camera_matrix,
        dist_coeffs
    )

    # Calculate reprojection error
    errors = []
    for sim_pt, proj_pt in zip(simulated_points, projected_points.reshape(-1, 2)):
        error = np.linalg.norm(sim_pt - proj_pt)
        errors.append(error)

    mean_error = np.mean(errors)
    std_error = np.std(errors)
    max_error = np.max(errors)

    print(f"Camera Calibration Validation:")
    print(f"  Mean Reprojection Error: {mean_error:.3f} pixels")
    print(f"  Std Dev: {std_error:.3f} pixels")
    print(f"  Max Error: {max_error:.3f} pixels")

    return {
        'mean_error': mean_error,
        'std_error': std_error,
        'max_error': max_error,
        'pass': mean_error < 2.0  # Pass if error < 2 pixels
    }
```

### Image Quality Assessment

```python
def assess_image_quality(simulated_image, reference_image):
    """Assess the quality of simulated camera images"""
    # Convert to grayscale for some metrics
    sim_gray = cv2.cvtColor(simulated_image, cv2.COLOR_RGB2GRAY)
    ref_gray = cv2.cvtColor(reference_image, cv2.COLOR_RGB2GRAY)

    # Calculate metrics
    results = {}

    # Peak Signal-to-Noise Ratio (PSNR)
    mse = np.mean((simulated_image.astype(np.float64) - reference_image.astype(np.float64)) ** 2)
    if mse == 0:
        results['psnr'] = float('inf')
    else:
        results['psnr'] = 20 * np.log10(255.0 / np.sqrt(mse))

    # Structural Similarity Index (SSIM)
    from skimage.metrics import structural_similarity
    results['ssim'] = structural_similarity(sim_gray, ref_gray, data_range=255)

    # Calculate histogram similarity
    hist_sim = calculate_histogram_similarity(simulated_image, reference_image)
    results['histogram_similarity'] = hist_sim

    return results

def calculate_histogram_similarity(img1, img2):
    """Calculate histogram-based similarity between images"""
    # Calculate histograms
    hist1 = cv2.calcHist([img1], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
    hist2 = cv2.calcHist([img2], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])

    # Normalize histograms
    hist1 = cv2.normalize(hist1, hist1).flatten()
    hist2 = cv2.normalize(hist2, hist2).flatten()

    # Calculate correlation
    correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)

    return correlation
```

## Common Issues and Troubleshooting

### Performance Issues

- **Slow Frame Rates**: Reduce resolution or processing complexity
- **High CPU Usage**: Use optimized libraries (Numba, OpenCV) and efficient algorithms
- **Memory Problems**: Process frames in batches and manage memory carefully

### Image Quality Issues

- **Wrong Colors**: Check color space conversions (RGB vs BGR)
- **Distortion Problems**: Verify distortion coefficients format and values
- **Exposure Issues**: Adjust exposure simulation parameters

### Integration Issues

- **TF Problems**: Ensure optical frame conventions are followed
- **Synchronization**: Camera images should be properly timestamped
- **Calibration**: Verify camera intrinsic parameters match real sensors

## Best Practices

### 1. Use Proper Optical Frame Conventions

Follow the standard optical frame convention: X-right, Y-down, Z-forward relative to the camera.

### 2. Include Realistic Optical Effects

Model lens distortion, exposure, and color effects based on real camera specifications.

### 3. Validate Against Real Data

Compare simulated camera outputs to real camera data when available.

### 4. Optimize for Performance

Balance image quality with computational requirements, especially for real-time applications.

### 5. Maintain Calibration Data

Keep accurate camera calibration parameters and ensure they're properly integrated.

## Summary

RGB camera simulation in Gazebo provides essential visual perception capabilities for robotics applications. By understanding camera optics, implementing proper distortion models, and simulating realistic optical effects, you can create high-fidelity camera simulations suitable for computer vision tasks. The integration with ROS 2 enables seamless use of simulated camera data in perception pipelines.

## References

1. Open Source Robotics Foundation. (2023). Gazebo Camera Sensors. https://gazebosim.org/tutorials?tut=ros_gzplugins_sensors
2. Zhang, Z. (2000). A Flexible New Technique for Camera Calibration. IEEE Transactions on Pattern Analysis and Machine Intelligence.
3. Hartley, R., & Zisserman, A. (2004). Multiple View Geometry in Computer Vision. Cambridge University Press.

## Exercises

1. Implement a camera simulation with realistic lens distortion
2. Create a calibration validation pipeline for simulated cameras
3. Develop performance optimization for real-time camera processing
4. Design a multi-camera stereo vision system simulation