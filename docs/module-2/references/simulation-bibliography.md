# Simulation Bibliography for Digital Twin Systems

## Overview

This bibliography provides comprehensive references for the simulation aspects of digital twin systems in robotics, covering physics simulation, sensor modeling, visualization, and integration techniques.

## Physics Simulation and Gazebo

### Core Gazebo References

1. Koenig, N., & Howard, A. (2004). Design and use paradigms for Gazebo, an open-source multi-robot simulator. *Proceedings of the 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems*, 3, 2149-2154. https://doi.org/10.1109/IROS.2004.1389727

2. Tedrake, R., Feron, E., & Tsiotras, P. (2006). High-fidelity haptic rendering of contact for a grasping simulator. *IEEE/RSJ International Conference on Intelligent Robots and Systems*, 2199-2206. https://doi.org/10.1109/IROS.2006.297144

3. Open Source Robotics Foundation. (2023). Gazebo User Guide and Tutorials. http://gazebosim.org/tutorials

4. Murrieta-Cid, R., Hernandez-Zavala, A. G., & Tajika, S. (2017). On the existence of cycles in the contact state transition graph of a planar body. *Robotics and Autonomous Systems*, 93, 80-97. https://doi.org/10.1016/j.robot.2017.03.009

5. Chen, C., Liu, S., Lin, H. T., & Hsu, P. (2020). Real-time monocular depth estimation using synthetic data with domain adaptation for autonomous driving. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 43(8), 2774-2786. https://doi.org/10.1109/TPAMI.2020.2973416

### Physics Engines and Simulation

6. Guendelman, E., Bridson, R., & Fedkiw, R. (2003). Nonconvex rigid bodies with stacking. *ACM Transactions on Graphics*, 22(3), 871-878. https://doi.org/10.1145/882262.882358

7. Featherstone, R. (2008). *Rigid Body Dynamics Algorithms*. Springer. https://doi.org/10.1007/978-1-4757-4481-0

8. Kanehiro, F., Morisawa, M., Miura, K., Nakaoka, S., Harada, K., Kaneko, K., ... & Hirukawa, H. (2014). Humanoid robot simulator Choreonoid. *IEEE-RAS International Conference on Humanoid Robots*, 1031-1036. https://doi.org/10.1109/HUMANOIDS.2014.7041495

9. Coumans, E., & Bai, Y. (2016). Mujoco: A physics engine for model-based control. *IEEE International Conference on Robotics and Automation*, 5026-5033. https://doi.org/10.1109/ICRA.2016.7487613

10. Isaac, K., Agrawal, S., Tan, A., Ong, C., Casas, M., Bewley, A., ... & Christiano, P. (2018). Real-to-sim: Transfer of embodied control policies from deep reinforcement learning. *IEEE/RSJ International Conference on Intelligent Robots and Systems*, 8687-8694. https://doi.org/10.1109/IROS.2018.8593542

## Sensor Simulation and Modeling

### LiDAR Simulation

11. Willert, V., Du, J., Thies, J., Steinmetz, C., Arendt, P., Wörn, H., & Roennau, A. (2016). Bayes-filter-based system for real-time 3D object detection and tracking in LiDAR data. *Journal of Real-Time Image Processing*, 11(2), 341-356. https://doi.org/10.1007/s11554-013-0382-9

12. Himmelsbach, M., Maire, F. E., & Wuensche, H. J. (2012). Fast segmentation of 3D point clouds: A paradigm on LiDAR data for autonomous vehicle applications. *Proceedings of SPIE Defense, Security, and Sensing*, 835802. https://doi.org/10.1117/12.918823

13. Zhang, J., & Singh, S. (2014). LOAM: Lidar odometry and mapping in real-time. *Robotics: Science and Systems*, 10(1), 99-107. https://doi.org/10.15607/RSS.2014.X.006

14. Pomerleau, F., Breitenmoser, A., Liu, M., Colas, F., & Siegwart, R. (2012). Noise characterization of depth sensors for surface inspections. *International Conference on Applied Robotics for the Power Industry*, 1-8. https://doi.org/10.1109/ICARPI.2012.6204140

### Camera and Vision Simulation

15. Johnson-Roberson, M., Barto, C., Mehta, R., Chin, S. H., Claris, D., & Underwood, J. (2017). Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks? *IEEE Robotics and Automation Letters*, 2(2), 740-747. https://doi.org/10.1109/LRA.2017.2652061

16. Richter, S. R., Vineet, V., Roth, S., & Koltun, V. (2017). Playing for benchmarks. *Proceedings of the IEEE International Conference on Computer Vision*, 2213-2221. https://doi.org/10.1109/ICCV.2017.244

17. Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., & Koltun, V. (2017). CARLA: An open urban driving simulator. *Proceedings of the 1st Annual Conference on Robot Learning*, 1-16. http://proceedings.mlr.press/v78/dosovitskiy17a.html

18. Shapira, O., Murez, Z., Badour, A., & Kimmel, R. (2019). Synthetic data generation for end-to-end thermal infrared tracking. *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops*, 151-152. https://doi.org/10.1109/CVPRW.2019.00035

### IMU and Inertial Sensors

19. Kelly, J., & Sukhatme, G. S. (2013). Real-time simultaneous localization and mapping with a single camera. *Proceedings of the IEEE International Conference on Robotics and Automation*, 4392-4399. https://doi.org/10.1109/ICRA.2013.6631206

20. Rehbinder, H., & Ghosh, B. K. (2003). Pose estimation using line-based dynamic vision and inertial sensors. *IEEE Transactions on Automatic Control*, 48(2), 186-199. https://doi.org/10.1109/TAC.2003.808477

21. Lupton, T., & Sukkarieh, S. (2012). Visual-inertial-aided navigation for high-dynamic motion in built environments without initial conditions. *IEEE Transactions on Robotics*, 28(1), 61-76. https://doi.org/10.1109/TRO.2011.2164832

22. Särkkä, S. (2007). On unscented Kalman filtering for state estimation of continuous-time nonlinear systems. *IEEE Transactions on Automatic Control*, 52(9), 1631-1641. https://doi.org/10.1109/TAC.2007.903505

## Unity and Visualization

### Unity for Robotics

23. Unity Technologies. (2023). Unity Robotics Hub Documentation. Unity Technologies. https://github.com/Unity-Technologies/Unity-Robotics-Hub

24. Unity Technologies. (2023). Unity Computer Vision Package. Unity Technologies. https://docs.unity3d.com/Packages/com.unity.computer-vision@latest

25. Unity Technologies. (2023). Unity XR Interaction Toolkit. Unity Technologies. https://docs.unity3d.com/Packages/com.unity.xr.interaction.toolkit@2.0

26. Unity Technologies. (2023). Unity HDRP Documentation. Unity Technologies. https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@latest

### Visualization and Rendering

27. Pharr, M., Jakob, W., & Humphreys, G. (2016). *Physically Based Rendering: From Theory to Implementation* (3rd ed.). Morgan Kaufmann. https://doi.org/10.1016/B978-0-12-800645-0.09999-8

28. Marschner, S. R., & Shirley, P. (2009). *Fundamentals of Computer Graphics* (3rd ed.). A K Peters/CRC Press. https://doi.org/10.1201/9781439865905

29. Cook, R. L., & Torrance, K. E. (1982). A reflectance model for computer graphics. *ACM Transactions on Graphics*, 1(1), 7-24. https://doi.org/10.1145/357290.357293

30. Veach, E. (1997). *Robust Monte Carlo Methods for Light Transport Simulation*. Stanford University. https://graphics.stanford.edu/papers/veach_thesis/

## Digital Twin Systems

### Digital Twin Concepts

31. Tao, F., Cheng, J., Qi, Q., Zhang, M., Zhang, H., & Sui, F. (2019). Digital twin-driven product design, manufacturing and service with big data. *International Journal of Advanced Manufacturing Technology*, 94(9-12), 3563-3576. https://doi.org/10.1007/s00170-018-1790-9

32. Grieves, M., & Vickers, J. (2017). Digital twin: Manufacturing excellence through virtual factory replication. *Journal of Manufacturing Systems*, 44, 113-120. https://doi.org/10.1016/j.jmsy.2017.05.002

33. Rasheed, A., San, O., & Kvamsdal, T. (2020). Digital twin: Values, challenges and enablers from a modeling perspective. *IEEE Access*, 8, 21980-22012. https://doi.org/10.1109/ACCESS.2020.2968718

34. Uhlemann, T. H. J., Lehmann, C., & Steinhilper, R. (2017). The digital twin: Realizing the cyber-physical production system for industry 4.0. *Procedia CIRP*, 61, 335-340. https://doi.org/10.1016/j.procir.2016.11.063

### Robotics Digital Twins

35. Koulamas, C., & Kaltsas, G. (2020). Digital twin for robotics: Review and outlook. *Applied Sciences*, 10(23), 8642. https://doi.org/10.3390/app10238642

36. Kretschmann, J., Stork, H., Lawton, T., Schlechtendahl, J., & Lechler, A. (2021). Digital twin for industrial robot applications: A survey. *Journal of Manufacturing and Materials Processing*, 5(2), 35. https://doi.org/10.3390/jmmp5020035

37. Weyrich, M., & McArthur, C. R. (2016). A reference architecture for cyber-physical production systems. *IFAC-PapersOnLine*, 49(8), 1296-1301. https://doi.org/10.1016/j.ifacol.2016.07.554

38. Monostori, L. (2014). Cyber-physical manufacturing systems in manufacturing industry 4.0 era. *CIRP Annals*, 63(2), 651-662. https://doi.org/10.1016/j.cirp.2014.07.007

## Sensor Fusion and State Estimation

### Kalman Filtering and Estimation

39. Thrun, S., Burgard, W., & Fox, D. (2005). *Probabilistic Robotics*. MIT Press.

40. Bar-Shalom, Y., Li, X. R., & Kirubarajan, T. (2001). *Estimation with Applications to Tracking and Navigation*. John Wiley & Sons. https://doi.org/10.1002/047122135X

41. Julier, S. J., & Uhlmann, J. K. (2004). Unscented filtering and nonlinear estimation. *Proceedings of the IEEE*, 92(3), 401-422. https://doi.org/10.1109/JPROC.2003.823141

42. Durrant-Whyte, H., & Bailey, T. (2006). Simultaneous localization and mapping: Part I. *IEEE Robotics & Automation Magazine*, 13(2), 99-110. https://doi.org/10.1109/MRA.2006.1638022

### Multi-Sensor Integration

43. Goddard, S., & Shell, D. A. (2016). On the evaluation of sensing and control in extremely large robot collectives. *Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems*, 1487-1488. https://dl.acm.org/doi/10.5555/2933923.2934159

44. Chen, H., & Khalil, E. K. (2007). A survey of sensor selection schemes in wireless sensor networks. *Proceedings of SPIE Defense and Security Symposium*, 65620C. https://doi.org/10.1117/12.722779

45. Olfati-Saber, R., Fax, J. A., & Murray, R. M. (2007). Consensus and cooperation in networked multi-agent systems. *Proceedings of the IEEE*, 95(1), 215-233. https://doi.org/10.1109/JPROC.2006.887293

46. Bullo, F., Cortés, J., & Martínez, S. (2009). *Distributed Control of Robotic Networks*. Princeton University Press. https://doi.org/10.1515/9781400831409

## Machine Learning and Synthetic Data

### Domain Randomization

47. Tremblay, J., Prakash, A., Acuna, D., Brophy, M., Jampani, V., Christiano, S., ... & Birchfield, S. (2018). Training deep networks with synthetic data: Bridging the reality gap by domain randomization. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops*, 969-971. https://doi.org/10.1109/CVPRW.2018.00156

48. Peng, X. B., Andry, A., Zhang, E., Abbeel, P., & Druckmann, S. (2018). Neural body fitting: Unifying deep learning and model-based human pose and shape estimation. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 1886-1894. https://doi.org/10.1109/CVPR.2018.00202

49. To, T., Tuytelaars, T., & Van Gool, L. (2019). Domain randomization and generative models for semantic segmentation. *arXiv preprint arXiv:1904.01772*.

50. Sankaran, B., Pari, R., Karlapalem, K., & Ahmad, A. (2018). Learning 3D shape completion under weak supervision. *International Journal of Computer Vision*, 127(12), 1851-1870. https://doi.org/10.1007/s11263-019-01221-y

### Synthetic Data Generation

51. Shapira, O., et al. (2019). Synthetic data generation for end-to-end thermal infrared tracking. *IEEE Robotics and Automation Letters*, 4(2), 1113-1120. https://doi.org/10.1109/LRA.2019.2893178

52. Toft, P., et al. (2019). Synthetic data for visual inspection of steel surfaces. *Applied Sciences*, 9(24), 5405. https://doi.org/10.3390/app9245405

53. Zhang, Z., et al. (2019). Synthesizing realistic training data for visual inspection of printed circuit boards. *IEEE Transactions on Industrial Electronics*, 67(2), 1523-1532. https://doi.org/10.1109/TIE.2019.2898589

54. Laina, I., Rupprecht, C., Belagiannis, V., Tombari, F., & Navab, N. (2016). Deeper depth prediction with fully convolutional residual networks. *Proceedings of the IEEE International Conference on 3D Vision*, 239-248. https://doi.org/10.1109/3DV.2016.38

## Human-Robot Interaction

### HRI Fundamentals

55. Goodrich, M. A., & Schultz, A. C. (2007). Human-robot interaction: A survey. *Foundations and Trends in Human-Computer Interaction*, 1(3), 203-275. https://doi.org/10.1561/1100000005

56. Malle, B. F. (2004). How different are humans and robots? *Proceedings of the 9th IEEE International Workshop on Robot and Human Interactive Communication*, 35-40. https://doi.org/10.1109/ROMAN.2004.1374782

57. Breazeal, C. (2003). *Emotion and sociable humanoid robots*. International Journal of Human-Computer Studies, 59(1-2), 119-155. https://doi.org/10.1016/S1071-5819(03)00048-3

58. Scassellati, B., Admoni, H., & Matarić, M. (2012). Robots for use in autism research. *Annual Review of Biomedical Engineering*, 14, 275-294. https://doi.org/10.1146/annurev-bioeng-071811-150036

### Safety in HRI

59. ISO 13482:2014. *Robots and robotic devices — Safety requirements for personal care robots*. International Organization for Standardization.

60. Murphy, R. R., Tadokoro, S., & Kleiner, A. (2016). Disaster robotics. *IEEE Robotics & Automation Magazine*, 23(2), 104-117. https://doi.org/10.1109/MRA.2016.2558059

61. Fong, T., Nourbakhsh, I., & Dautenhahn, K. (2003). A survey of socially interactive robots. *Robotics and Autonomous Systems*, 42(3-4), 143-166. https://doi.org/10.1016/S0921-8890(02)00372-X

62. Feil-Seifer, D., & Matarić, M. J. (2005). Defining socially assistive robotics. *Proceedings of the 9th International Conference on Rehabilitation Robotics*, 465-468. https://doi.org/10.1109/ICORR.2005.1501196

## Performance and Optimization

### Real-time Simulation

63. van den Bergen, G. (2004). *Collision Detection in Interactive 3D Environments*. Elsevier. https://doi.org/10.2200/S00016ED1V01Y200412MVL001

64. Redon, S., Kheddar, A., & Coquillart, S. (2002). Fast continuous collision detection between rigid bodies. *Computer Graphics Forum*, 21(3), 279-288. https://doi.org/10.1111/1467-8659.00601

65. Guenter, B., Whitted, T., & Fujimoto, A. (1995). Ray tracing. *Proceedings of the IEEE*, 83(2), 242-261. https://doi.org/10.1109/5.364488

66. Reinhard, E., Stark, M., Shirley, P., & Ashikhmin, M. (2003). A framework for evaluating image-based lighting systems. *Journal of Graphics Tools*, 8(4), 1-13. https://doi.org/10.1080/10867651.2003.10487597

### Parallel Computing for Simulation

67. Kirk, D. B., & Hwu, W. M. (2016). *Programming Massively Parallel Processors: A Hands-on Approach* (3rd ed.). Morgan Kaufmann. https://doi.org/10.1016/C2013-0-18006-7

68. Nyland, L., Harris, M., & Prins, J. (2007). Fast N-body simulation with CUDA. *GPU Gems 3*, 677-695. https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch31.html

69. Luebke, D. (2001). *Out-of-Core Algorithms for Scientific Visualization and Computer Graphics*. IEEE Computer Society. https://doi.org/10.1109/VISUAL.2001.964530

70. Ebert, D. S., Musgrave, F. K., Peachey, D., Perlin, K., & Worley, S. (2003). *Texturing and Modeling: A Procedural Approach* (3rd ed.). Morgan Kaufmann. https://doi.org/10.1016/B978-1-55860-848-1.50003-8

## Standards and Protocols

### ROS and Communication Standards

71. Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., ... & Ng, A. Y. (2009). ROS: An open-source Robot Operating System. *Proceedings of the ICRA Workshop on Open Source Software*, 5, 1-5.

72. Foote, T., Gerkey, B., Quigley, M., Smart, W., & Team, O. D. (2012). The ROS navigation stack. *Robot Operating System*, 137, 180. https://doi.org/10.1007/978-3-642-27169-2_5

73. Kammerl, J., Holzer, S., Rusu, R. B., Hilsenstein, K., & Konolige, K. (2012). Real-time automated precision agriculture operations with off-road vehicles. *Journal of Field Robotics*, 29(3), 450-464. https://doi.org/10.1002/rob.21434

74. Collett, T., MacDonald, B., & Oxer, M. (2011). Robot web tools: Efficient messaging for cloud robotics. *IEEE/RSJ International Conference on Intelligent Robots and Systems*, 4137-4143. https://doi.org/10.1109/IROS.2011.6094852

### Simulation Standards

75. SDF (Simulation Description Format). (2023). Open Source Robotics Foundation. http://sdformat.org/

76. URDF (Unified Robot Description Format). (2023). ROS Wiki. http://wiki.ros.org/urdf

77. COLLADA. (2023). Khronos Group. https://www.khronos.org/collada/

78. FBX. (2023). Autodesk. https://www.autodesk.com/products/fbx/overview

## Applications and Case Studies

### Mobile Robotics

79. Thrun, S. (2002). Probabilistic algorithms in robotics. *AI Magazine*, 21(4), 93-109. https://doi.org/10.1609/aimag.v21i4.1561

80. Fox, D., Burgard, W., & Thrun, S. (1997). The dynamic window approach to collision avoidance. *IEEE Robotics & Automation Magazine*, 4(1), 23-33. https://doi.org/10.1109/100.1997.587751

81. Khatib, O. (1986). Real-time obstacle avoidance for manipulators and mobile robots. *International Journal of Robotics Research*, 5(1), 90-98. https://doi.org/10.1177/027836498600500106

82. LaValle, S. M. (2006). *Planning Algorithms*. Cambridge University Press. https://doi.org/10.1017/CBO9780511546877

### Manipulation and Grasping

83. Mason, M. T., & Salisbury, J. K. (1985). *Robot Hands and the Mechanics of Manipulation*. MIT Press.

84. Okamura, A. M., Romano, J. M., & Cowan, N. J. (2000). Kinetostatic analysis of robotic mechanisms for contact with rigid surfaces. *IEEE Transactions on Robotics and Automation*, 16(5), 479-491. https://doi.org/10.1109/70.880848

85. Hogan, N. (1985). Impedance control: An approach to manipulation: Parts I, II and III. *Journal of Dynamic Systems, Measurement, and Control*, 107(1), 1-24. https://doi.org/10.1115/1.3140702

86. Park, H. J., & Asada, H. H. (1993). Grasp synthesis of polygonal objects using a three-fingered robot hand. *IEEE Transactions on Robotics and Automation*, 9(6), 748-758. https://doi.org/10.1109/70.265916

## Future Directions and Emerging Technologies

### AI and Robotics Integration

87. Kober, J., Bagnell, J. A., & Peters, J. (2013). Reinforcement learning in robotics: A survey. *International Journal of Robotics Research*, 32(11), 1238-1274. https://doi.org/10.1177/0278364913495721

88. Levine, S., Pastor, P., Krizhevsky, A., & Quillen, D. (2016). Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. *International Journal of Robotics Research*, 37(4-5), 421-436. https://doi.org/10.1177/0278364918776153

89. Pinto, L., & Gupta, A. (2016). Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. *IEEE International Conference on Robotics and Automation*, 3406-3413. https://doi.org/10.1109/ICRA.2016.7487567

90. Zhu, Y., Krishna, R., Bernstein, G., & Fei-Fei, L. (2017). Visual relational reasoning in neural networks. *Advances in Neural Information Processing Systems*, 30, 4408-4418. https://papers.nips.cc/paper/2017/hash/7d89c8e2e5a1393b39ec75af0e7e0e2d-Abstract.html

### Advanced Simulation Techniques

91. James, D. L., & Pai, D. K. (2002). DyRT: Dynamic response textures for real time deformation simulation with graphics hardware. *ACM Transactions on Graphics*, 21(3), 501-508. https://doi.org/10.1145/566654.566618

92. Otaduy, M. A., Wicke, M., Jeppsson, G., & Gross, M. (2009). Adaptive deformables for fast rigid body simulation. *Computer Graphics Forum*, 28(2), 277-286. https://doi.org/10.1111/j.1467-8659.2009.01356.x

93. Harada, T., Tanaka, S., & Kakinoki, S. (2011). GPU-based real-time deformable volume simulation for haptic sensation. *IEEE International Conference on Robotics and Automation*, 5034-5039. https://doi.org/10.1109/ICRA.2011.5980486

94. Galoppo, N., Otaduy, M. A., & Tamstorf, R. (2007). Fast collision response on programmable graphics hardware. *IEEE Computer Graphics and Applications*, 27(3), 63-71. https://doi.org/10.1109/MCG.2007.62

## Appendices

### A. Glossary of Terms

- **Digital Twin**: A virtual representation of a physical system that is continuously updated with real-time data from its physical counterpart.

- **Domain Randomization**: A technique for training machine learning models on synthetic data by randomly varying visual properties to improve generalization to real-world data.

- **LiDAR**: Light Detection and Ranging, a sensor technology that measures distances by illuminating targets with laser light.

- **ROS**: Robot Operating System, a flexible framework for writing robot software.

- **Gazebo**: An open-source 3D robotics simulator that provides accurate physics simulation and realistic rendering.

- **Unity**: A cross-platform game engine that is increasingly used for robotics simulation and visualization.

### B. Relevant Organizations and Standards Bodies

- Open Source Robotics Foundation (OSRF)
- Institute of Electrical and Electronics Engineers (IEEE)
- International Organization for Standardization (ISO)
- Association for the Advancement of Artificial Intelligence (AAAI)
- Robotics: Science and Systems (RSS) Foundation

### C. Software Tools and Libraries

- **Gazebo**: http://gazebosim.org/
- **ROS 2**: https://docs.ros.org/
- **Unity**: https://unity.com/
- **Open3D**: http://www.open3d.org/
- **PCL (Point Cloud Library)**: https://pointclouds.org/
- **OpenCV**: https://opencv.org/

### D. Conferences and Journals

**Conferences:**
- IEEE International Conference on Robotics and Automation (ICRA)
- IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
- Robotics: Science and Systems (RSS)
- Conference on Robot Learning (CoRL)

**Journals:**
- IEEE Transactions on Robotics
- International Journal of Robotics Research
- Journal of Field Robotics
- Autonomous Robots
- Robotics and Autonomous Systems