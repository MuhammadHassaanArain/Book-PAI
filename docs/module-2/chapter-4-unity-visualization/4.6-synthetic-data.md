# 4.6 Synthetic Data Generation for Vision Models

## Introduction

Synthetic data generation is a transformative approach in robotics and computer vision, enabling the creation of large-scale, diverse, and perfectly annotated datasets without the need for expensive real-world data collection. Unity provides powerful capabilities for generating synthetic data that can be used to train machine learning models for various vision tasks, including object detection, segmentation, depth estimation, and pose estimation. This chapter explores the implementation of synthetic data generation pipelines in Unity, covering rendering techniques, annotation methods, and integration with machine learning frameworks.

## Fundamentals of Synthetic Data Generation

### The Need for Synthetic Data

Traditional computer vision datasets require extensive real-world data collection, which is time-consuming, expensive, and often limited in diversity. Synthetic data generation addresses these challenges by:

1. **Unlimited Scalability**: Generate thousands or millions of images with minimal cost
2. **Perfect Annotations**: Access to ground truth data for all objects and properties
3. **Controlled Variability**: Systematically vary lighting, pose, background, and other factors
4. **Safety**: Generate data for dangerous or rare scenarios without risk
5. **Privacy**: Avoid privacy concerns associated with real-world data collection

### Types of Synthetic Data

Unity can generate various types of synthetic data for vision applications:

#### RGB Images
- Photorealistic color images for object detection and classification
- Various lighting conditions and environmental settings
- Different camera perspectives and parameters

#### Depth Maps
- Accurate depth information for 3D understanding
- Precise geometric relationships between objects
- Ground truth for depth estimation models

#### Semantic Segmentation Masks
- Pixel-perfect class labels for each object
- Material-specific segmentation
- Instance segmentation with unique identifiers

#### Normal Maps
- Surface orientation information for geometry understanding
- Detailed geometric information for reconstruction tasks
- Surface property estimation

#### Optical Flow
- Motion vectors for temporal understanding
- Scene flow for 3D motion analysis
- Temporal consistency for video applications

## Unity Rendering Techniques for Synthetic Data

### Multi-Pass Rendering

Unity's rendering pipeline can be configured to generate multiple data types simultaneously:

```csharp
using UnityEngine;
using System.Collections;
using System.IO;

public class SyntheticDataRenderer : MonoBehaviour
{
    [Header("Camera Configuration")]
    public Camera rgbCamera;
    public Camera depthCamera;
    public Camera segmentationCamera;
    public Camera normalCamera;

    [Header("Output Configuration")]
    public string outputDirectory = "SyntheticData";
    public int imageWidth = 640;
    public int imageHeight = 480;
    public int antiAliasing = 1;
    public bool generateRGB = true;
    public bool generateDepth = true;
    public bool generateSegmentation = true;
    public bool generateNormals = true;

    [Header("Dataset Configuration")]
    public int numberOfScenes = 1000;
    public int imagesPerScene = 10;
    public float sceneVariationInterval = 5.0f;

    [Header("Object Configuration")]
    public GameObject[] objectsToRandomize;
    public Material[] materialsToApply;
    public Light[] lightsToAdjust;
    public Color[] segmentationColors;

    private RenderTexture rgbTexture;
    private RenderTexture depthTexture;
    private RenderTexture segmentationTexture;
    private RenderTexture normalTexture;

    void Start()
    {
        SetupCameras();
        CreateRenderTextures();
        StartCoroutine(GenerateDataset());
    }

    void SetupCameras()
    {
        // Configure RGB camera
        if (rgbCamera != null)
        {
            rgbCamera.targetTexture = rgbTexture;
            rgbCamera.backgroundColor = Color.black;
            rgbCamera.clearFlags = CameraClearFlags.SolidColor;
        }

        // Configure depth camera with depth texture
        if (depthCamera != null)
        {
            depthCamera.depthTextureMode = DepthTextureMode.Depth;
            depthCamera.backgroundColor = Color.white;
            depthCamera.clearFlags = CameraClearFlags.SolidColor;
        }

        // Configure segmentation camera with unique colors
        if (segmentationCamera != null)
        {
            segmentationCamera.backgroundColor = Color.black;
            segmentationCamera.clearFlags = CameraClearFlags.SolidColor;
        }

        // Configure normal camera
        if (normalCamera != null)
        {
            normalCamera.depthTextureMode = DepthTextureMode.DepthNormals;
            normalCamera.backgroundColor = Color.black;
            normalCamera.clearFlags = CameraClearFlags.SolidColor;
        }
    }

    void CreateRenderTextures()
    {
        // Create RGB texture
        if (generateRGB)
        {
            rgbTexture = new RenderTexture(imageWidth, imageHeight, 24, RenderTextureFormat.ARGB32);
            rgbTexture.antiAliasing = antiAliasing;
            rgbTexture.Create();
        }

        // Create depth texture
        if (generateDepth)
        {
            depthTexture = new RenderTexture(imageWidth, imageHeight, 24, RenderTextureFormat.RFloat);
            depthTexture.Create();
        }

        // Create segmentation texture
        if (generateSegmentation)
        {
            segmentationTexture = new RenderTexture(imageWidth, imageHeight, 0, RenderTextureFormat.ARGB32);
            segmentationTexture.Create();
        }

        // Create normal texture
        if (generateNormals)
        {
            normalTexture = new RenderTexture(imageWidth, imageHeight, 24, RenderTextureFormat.ARGB32);
            normalTexture.Create();
        }
    }

    IEnumerator GenerateDataset()
    {
        for (int scene = 0; scene < numberOfScenes; scene++)
        {
            // Randomize scene configuration
            RandomizeSceneConfiguration();

            for (int img = 0; img < imagesPerScene; img++)
            {
                // Move camera to random position
                MoveCameraToRandomPosition();

                // Capture all data types
                if (generateRGB)
                    CaptureRGBImage(scene, img);

                if (generateDepth)
                    CaptureDepthImage(scene, img);

                if (generateSegmentation)
                    CaptureSegmentationMask(scene, img);

                if (generateNormals)
                    CaptureNormalMap(scene, img);

                // Save metadata
                SaveMetadata(scene, img);

                yield return null; // Wait for next frame
            }

            // Randomize scene after interval
            yield return new WaitForSeconds(sceneVariationInterval);
        }

        Debug.Log("Dataset generation completed!");
    }

    void RandomizeSceneConfiguration()
    {
        // Randomize object positions
        foreach (var obj in objectsToRandomize)
        {
            Vector3 randomPos = new Vector3(
                Random.Range(-10f, 10f),
                Random.Range(0.1f, 5f),
                Random.Range(-10f, 10f)
            );
            obj.transform.position = randomPos;

            // Randomize object rotation
            obj.transform.rotation = Quaternion.Euler(
                Random.Range(0f, 360f),
                Random.Range(0f, 360f),
                Random.Range(0f, 360f)
            );

            // Randomize material
            Renderer renderer = obj.GetComponent<Renderer>();
            if (renderer != null && materialsToApply.Length > 0)
            {
                Material randomMaterial = materialsToApply[Random.Range(0, materialsToApply.Length)];
                renderer.material = randomMaterial;
            }
        }

        // Randomize lighting
        foreach (var light in lightsToAdjust)
        {
            light.intensity = Random.Range(0.5f, 2f);
            light.color = Random.ColorHSV(0f, 1f, 0.5f, 1f, 0.5f, 1f);

            // Randomize light position and rotation
            light.transform.position = new Vector3(
                Random.Range(-5f, 5f),
                Random.Range(3f, 8f),
                Random.Range(-5f, 5f)
            );
        }
    }

    void MoveCameraToRandomPosition()
    {
        Vector3 randomOffset = new Vector3(
            Random.Range(-8f, 8f),
            Random.Range(1f, 6f),
            Random.Range(-8f, 8f)
        );
        transform.position = randomOffset;

        // Look at a random point in the scene
        Vector3 lookTarget = new Vector3(
            Random.Range(-3f, 3f),
            Random.Range(1f, 3f),
            Random.Range(-3f, 3f)
        );
        transform.LookAt(lookTarget);
    }

    void CaptureRGBImage(int scene, int img)
    {
        string filename = Path.Combine(
            outputDirectory,
            "rgb",
            $"scene_{scene:0000}_img_{img:0000}.png"
        );

        CaptureCameraImage(rgbCamera, filename);
    }

    void CaptureDepthImage(int scene, int img)
    {
        string filename = Path.Combine(
            outputDirectory,
            "depth",
            $"scene_{scene:0000}_img_{img:0000}.exr"
        );

        CaptureDepthData(depthCamera, filename);
    }

    void CaptureSegmentationMask(int scene, int img)
    {
        string filename = Path.Combine(
            outputDirectory,
            "segmentation",
            $"scene_{scene:0000}_img_{img:0000}.png"
        );

        CaptureSegmentationData(segmentationCamera, filename);
    }

    void CaptureNormalMap(int scene, int img)
    {
        string filename = Path.Combine(
            outputDirectory,
            "normals",
            $"scene_{scene:0000}_img_{img:0000}.png"
        );

        CaptureNormalData(normalCamera, filename);
    }

    void CaptureCameraImage(Camera cam, string filename)
    {
        RenderTexture currentRT = RenderTexture.active;
        RenderTexture.active = cam.targetTexture;

        cam.Render();

        Texture2D image = new Texture2D(cam.targetTexture.width, cam.targetTexture.height, TextureFormat.RGB24, false);
        image.ReadPixels(new Rect(0, 0, cam.targetTexture.width, cam.targetTexture.height), 0, 0);
        image.Apply();

        RenderTexture.active = currentRT;

        // Create directory if it doesn't exist
        Directory.CreateDirectory(Path.GetDirectoryName(filename));

        // Save image
        byte[] bytes = image.EncodeToPNG();
        File.WriteAllBytes(filename, bytes);

        DestroyImmediate(image);
    }

    void CaptureDepthData(Camera cam, string filename)
    {
        // Render depth data
        RenderTexture currentRT = RenderTexture.active;
        RenderTexture.active = cam.targetTexture;
        cam.Render();

        Texture2D depthTexture2D = new Texture2D(cam.targetTexture.width, cam.targetTexture.height, TextureFormat.RFloat, false);
        depthTexture2D.ReadPixels(new Rect(0, 0, cam.targetTexture.width, cam.targetTexture.height), 0, 0);
        depthTexture2D.Apply();

        RenderTexture.active = currentRT;

        // Convert to proper depth format and save
        Directory.CreateDirectory(Path.GetDirectoryName(filename));

        // For depth data, we typically save as EXR for higher precision
        byte[] bytes = depthTexture2D.EncodeToEXR();
        File.WriteAllBytes(filename, bytes);

        DestroyImmediate(depthTexture2D);
    }

    void CaptureSegmentationData(Camera cam, string filename)
    {
        // For segmentation, we render objects with unique colors
        // This requires special shaders or materials for each object class
        RenderTexture currentRT = RenderTexture.active;
        RenderTexture.active = cam.targetTexture;
        cam.Render();

        Texture2D segmentationTexture2D = new Texture2D(cam.targetTexture.width, cam.targetTexture.height, TextureFormat.RGB24, false);
        segmentationTexture2D.ReadPixels(new Rect(0, 0, cam.targetTexture.width, cam.targetTexture.height), 0, 0);
        segmentationTexture2D.Apply();

        RenderTexture.active = currentRT;

        Directory.CreateDirectory(Path.GetDirectoryName(filename));

        byte[] bytes = segmentationTexture2D.EncodeToPNG();
        File.WriteAllBytes(filename, bytes);

        DestroyImmediate(segmentationTexture2D);
    }

    void CaptureNormalData(Camera cam, string filename)
    {
        RenderTexture currentRT = RenderTexture.active;
        RenderTexture.active = cam.targetTexture;
        cam.Render();

        Texture2D normalTexture2D = new Texture2D(cam.targetTexture.width, cam.targetTexture.height, TextureFormat.RGB24, false);
        normalTexture2D.ReadPixels(new Rect(0, 0, cam.targetTexture.width, cam.targetTexture.height), 0, 0);
        normalTexture2D.Apply();

        RenderTexture.active = currentRT;

        Directory.CreateDirectory(Path.GetDirectoryName(filename));

        byte[] bytes = normalTexture2D.EncodeToPNG();
        File.WriteAllBytes(filename, bytes);

        DestroyImmediate(normalTexture2D);
    }

    void SaveMetadata(int scene, int img)
    {
        string metadataPath = Path.Combine(
            outputDirectory,
            "metadata",
            $"scene_{scene:0000}_img_{img:0000}.json"
        );

        Directory.CreateDirectory(Path.GetDirectoryName(metadataPath));

        // Create metadata object
        SyntheticDataMetadata metadata = new SyntheticDataMetadata
        {
            sceneId = scene,
            imageId = img,
            timestamp = System.DateTime.UtcNow.ToString("o"),
            cameraPosition = transform.position,
            cameraRotation = transform.rotation.eulerAngles,
            objects = GetObjectMetadata()
        };

        string json = JsonUtility.ToJson(metadata, true);
        File.WriteAllText(metadataPath, json);
    }

    ObjectMetadata[] GetObjectMetadata()
    {
        ObjectMetadata[] objects = new ObjectMetadata[objectsToRandomize.Length];

        for (int i = 0; i < objectsToRandomize.Length; i++)
        {
            GameObject obj = objectsToRandomize[i];
            objects[i] = new ObjectMetadata
            {
                name = obj.name,
                position = obj.transform.position,
                rotation = obj.transform.rotation.eulerAngles,
                scale = obj.transform.localScale,
                material = obj.GetComponent<Renderer>()?.material?.name ?? "Unknown"
            };
        }

        return objects;
    }
}

[System.Serializable]
public class SyntheticDataMetadata
{
    public int sceneId;
    public int imageId;
    public string timestamp;
    public Vector3 cameraPosition;
    public Vector3 cameraRotation;
    public ObjectMetadata[] objects;
}

[System.Serializable]
public class ObjectMetadata
{
    public string name;
    public Vector3 position;
    public Vector3 rotation;
    public Vector3 scale;
    public string material;
}
```

## Advanced Synthetic Data Techniques

### Domain Randomization

Domain randomization is a powerful technique for improving model generalization by systematically varying visual properties:

```csharp
using UnityEngine;
using System.Collections.Generic;

public class DomainRandomizer : MonoBehaviour
{
    [Header("Lighting Randomization")]
    public float minLightIntensity = 0.5f;
    public float maxLightIntensity = 2.0f;
    public float minLightTemperature = 3200f; // Kelvin
    public float maxLightTemperature = 6500f; // Kelvin

    [Header("Material Randomization")]
    public float minRoughness = 0.1f;
    public float maxRoughness = 0.9f;
    public float minMetallic = 0.0f;
    public float maxMetallic = 1.0f;

    [Header("Background Randomization")]
    public Texture2D[] backgroundTextures;
    public Color[] skyboxColors;

    [Header("Camera Randomization")]
    public float minFOV = 30f;
    public float maxFOV = 90f;
    public float minAperture = 1.4f;
    public float maxAperture = 16f;

    [Header("Object Placement Randomization")]
    public float minObjectScale = 0.5f;
    public float maxObjectScale = 2.0f;
    public float minObjectSpacing = 0.5f;
    public float maxObjectSpacing = 3.0f;

    [Header("Atmospheric Effects")]
    public float minFogDensity = 0.0f;
    public float maxFogDensity = 0.1f;
    public Color minFogColor = Color.white;
    public Color maxFogColor = Color.gray;

    private Dictionary<Material, MaterialProperties> originalMaterials = new Dictionary<Material, MaterialProperties>();

    [System.Serializable]
    public class MaterialProperties
    {
        public float roughness;
        public float metallic;
        public Color color;
        public Texture mainTexture;
    }

    void Start()
    {
        CacheOriginalMaterials();
    }

    void CacheOriginalMaterials()
    {
        Renderer[] renderers = FindObjectsOfType<Renderer>();
        foreach (Renderer renderer in renderers)
        {
            foreach (Material mat in renderer.sharedMaterials)
            {
                if (!originalMaterials.ContainsKey(mat))
                {
                    MaterialProperties props = new MaterialProperties
                    {
                        roughness = GetMaterialProperty(mat, "_Smoothness"),
                        metallic = GetMaterialProperty(mat, "_Metallic"),
                        color = GetMaterialColor(mat, "_Color"),
                        mainTexture = mat.GetTexture("_MainTex")
                    };
                    originalMaterials[mat] = props;
                }
            }
        }
    }

    float GetMaterialProperty(Material mat, string propertyName)
    {
        if (mat.HasProperty(propertyName))
            return mat.GetFloat(propertyName);
        return 0f;
    }

    Color GetMaterialColor(Material mat, string propertyName)
    {
        if (mat.HasProperty(propertyName))
            return mat.GetColor(propertyName);
        return Color.white;
    }

    public void RandomizeScene()
    {
        RandomizeLighting();
        RandomizeMaterials();
        RandomizeBackground();
        RandomizeCamera();
        RandomizeObjectPlacement();
        RandomizeAtmosphericEffects();
    }

    void RandomizeLighting()
    {
        Light[] lights = FindObjectsOfType<Light>();
        foreach (Light light in lights)
        {
            // Randomize intensity
            light.intensity = Random.Range(minLightIntensity, maxLightIntensity);

            // Randomize color temperature
            float temperature = Random.Range(minLightTemperature, maxLightTemperature);
            light.color = GetColorFromTemperature(temperature);

            // Randomize position and rotation for moving lights
            if (light.type == LightType.Point || light.type == LightType.Spot)
            {
                light.transform.position = new Vector3(
                    Random.Range(-10f, 10f),
                    Random.Range(2f, 8f),
                    Random.Range(-10f, 10f)
                );
            }
        }
    }

    Color GetColorFromTemperature(float temperature)
    {
        // Approximate color from temperature (simplified algorithm)
        float temp = temperature / 100f;
        float r, g, b;

        // Red
        if (temp <= 66)
            r = 255;
        else
        {
            r = temp - 60;
            r = 329.698727446f * Mathf.Pow(r, -0.1332047592f);
            r = Mathf.Clamp(r, 0, 255);
        }

        // Green
        if (temp <= 66)
        {
            g = temp;
            g = 99.4708025861f * Mathf.Log(g) - 161.1195681661f;
        }
        else
        {
            g = temp - 60;
            g = 288.1221695283f * Mathf.Pow(g, -0.0755148492f);
        }
        g = Mathf.Clamp(g, 0, 255);

        // Blue
        if (temp >= 66)
            b = 255;
        else if (temp <= 19)
            b = 0;
        else
        {
            b = temp - 10;
            b = 138.5177312231f * Mathf.Log(b) - 305.0447927307f;
            b = Mathf.Clamp(b, 0, 255);
        }

        return new Color(r / 255f, g / 255f, b / 255f);
    }

    void RandomizeMaterials()
    {
        Renderer[] renderers = FindObjectsOfType<Renderer>();
        foreach (Renderer renderer in renderers)
        {
            for (int i = 0; i < renderer.sharedMaterials.Length; i++)
            {
                Material mat = renderer.sharedMaterials[i];

                if (originalMaterials.ContainsKey(mat))
                {
                    MaterialProperties original = originalMaterials[mat];

                    // Create a new material instance to avoid affecting other objects
                    Material newMat = new Material(mat);
                    renderer.sharedMaterials[i] = newMat;

                    // Randomize surface properties
                    float roughness = Random.Range(minRoughness, maxRoughness);
                    newMat.SetFloat("_Smoothness", 1.0f - roughness); // Unity uses smoothness, not roughness

                    float metallic = Random.Range(minMetallic, maxMetallic);
                    newMat.SetFloat("_Metallic", metallic);

                    // Randomize color while maintaining some original hue
                    Color originalColor = original.color;
                    Color randomizedColor = new Color(
                        Mathf.Clamp01(originalColor.r + Random.Range(-0.2f, 0.2f)),
                        Mathf.Clamp01(originalColor.g + Random.Range(-0.2f, 0.2f)),
                        Mathf.Clamp01(originalColor.b + Random.Range(-0.2f, 0.2f)),
                        originalColor.a
                    );
                    newMat.SetColor("_Color", randomizedColor);
                }
            }
        }
    }

    void RandomizeBackground()
    {
        // Randomize skybox color
        RenderSettings.fog = true;
        RenderSettings.fogColor = Color.Lerp(minFogColor, maxFogColor, Random.value);
        RenderSettings.fogDensity = Random.Range(minFogDensity, maxFogDensity);

        // Randomize background texture if using a custom shader
        if (backgroundTextures.Length > 0)
        {
            // This would require custom implementation based on your background setup
        }
    }

    void RandomizeCamera()
    {
        Camera cam = Camera.main;
        if (cam != null)
        {
            // Randomize field of view
            cam.fieldOfView = Random.Range(minFOV, maxFOV);

            // Note: Aperture is not directly controllable in Unity's default camera
            // This would require a custom camera script or post-processing effects
        }
    }

    void RandomizeObjectPlacement()
    {
        // Randomize object positions and scales
        GameObject[] objects = GameObject.FindGameObjectsWithTag("Randomizable");
        for (int i = 0; i < objects.Length; i++)
        {
            GameObject obj = objects[i];

            // Randomize position
            Vector3 randomPos = new Vector3(
                Random.Range(-5f, 5f),
                Random.Range(0.1f, 3f),
                Random.Range(-5f, 5f)
            );
            obj.transform.position = randomPos;

            // Randomize scale
            float scale = Random.Range(minObjectScale, maxObjectScale);
            obj.transform.localScale = Vector3.one * scale;
        }
    }

    void RandomizeAtmosphericEffects()
    {
        // Unity's fog settings are already handled in RandomizeBackground
        // Additional atmospheric effects could include:
        // - Volumetric fog
        // - Atmospheric scattering
        // - Weather effects
    }

    public void ResetToOriginal()
    {
        Renderer[] renderers = FindObjectsOfType<Renderer>();
        foreach (Renderer renderer in renderers)
        {
            for (int i = 0; i < renderer.sharedMaterials.Length; i++)
            {
                Material mat = renderer.sharedMaterials[i];
                if (originalMaterials.ContainsKey(mat))
                {
                    Material originalMat = GetOriginalMaterial(mat);
                    renderer.sharedMaterials[i] = originalMat;
                }
            }
        }
    }

    Material GetOriginalMaterial(Material modifiedMaterial)
    {
        // Find the original material based on name or other properties
        // This is a simplified approach - in practice, you might need more sophisticated matching
        foreach (var kvp in originalMaterials)
        {
            if (kvp.Key.name == modifiedMaterial.name)
            {
                return kvp.Key;
            }
        }
        return modifiedMaterial; // Return self if original not found
    }
}
```

### Physics-Based Data Generation

Incorporating realistic physics into synthetic data generation:

```csharp
using UnityEngine;
using System.Collections;

public class PhysicsBasedDataGenerator : MonoBehaviour
{
    [Header("Physics Configuration")]
    public float simulationDuration = 2.0f;
    public int framesPerSimulation = 30;
    public bool enablePhysics = true;
    public float gravityScale = 1.0f;

    [Header("Object Configuration")]
    public GameObject[] physicsObjects;
    public float minInitialVelocity = 0.1f;
    public float maxInitialVelocity = 2.0f;
    public float minTorque = 0.1f;
    public float maxTorque = 1.0f;

    [Header("Capture Configuration")]
    public bool captureTrajectory = true;
    public bool captureCollisionEvents = true;
    public bool captureContactPoints = true;

    private PhysicsSimulationState[] simulationStates;
    private int currentFrame = 0;

    [System.Serializable]
    public class PhysicsSimulationState
    {
        public float time;
        public Vector3[] positions;
        public Quaternion[] rotations;
        public Vector3[] velocities;
        public Vector3[] angularVelocities;
        public bool[] isColliding;
    }

    void Start()
    {
        Physics.gravity *= gravityScale;
        StartCoroutine(GeneratePhysicsBasedData());
    }

    IEnumerator GeneratePhysicsBasedData()
    {
        for (int sim = 0; sim < framesPerSimulation; sim++)
        {
            // Reset physics objects to initial state
            ResetPhysicsObjects();

            // Apply random initial forces
            ApplyRandomInitialForces();

            // Run physics simulation
            float simulationTime = 0f;
            float timeStep = simulationDuration / framesPerSimulation;

            while (simulationTime < simulationDuration)
            {
                // Capture state at regular intervals
                if (captureTrajectory)
                {
                    CapturePhysicsState(simulationTime);
                }

                // Simulate physics
                if (enablePhysics)
                {
                    yield return new WaitForFixedUpdate();
                    simulationTime += Time.fixedDeltaTime;
                }
                else
                {
                    simulationTime += timeStep;
                    yield return new WaitForSeconds(timeStep);
                }
            }

            // Capture final state
            if (captureTrajectory)
            {
                CapturePhysicsState(simulationDuration);
            }

            yield return null; // Allow other processes
        }

        Debug.Log("Physics-based data generation completed!");
    }

    void ResetPhysicsObjects()
    {
        for (int i = 0; i < physicsObjects.Length; i++)
        {
            GameObject obj = physicsObjects[i];

            // Reset position
            obj.transform.position = new Vector3(
                Random.Range(-2f, 2f),
                Random.Range(3f, 6f),
                Random.Range(-2f, 2f)
            );

            // Reset rotation
            obj.transform.rotation = Quaternion.Euler(
                Random.Range(0f, 360f),
                Random.Range(0f, 360f),
                Random.Range(0f, 360f)
            );

            // Reset physics properties
            Rigidbody rb = obj.GetComponent<Rigidbody>();
            if (rb != null)
            {
                rb.velocity = Vector3.zero;
                rb.angularVelocity = Vector3.zero;
                rb.Sleep(); // Put to sleep initially
            }
        }
    }

    void ApplyRandomInitialForces()
    {
        foreach (GameObject obj in physicsObjects)
        {
            Rigidbody rb = obj.GetComponent<Rigidbody>();
            if (rb != null)
            {
                // Apply random linear velocity
                Vector3 randomVelocity = new Vector3(
                    Random.Range(-maxInitialVelocity, maxInitialVelocity),
                    Random.Range(-maxInitialVelocity, maxInitialVelocity),
                    Random.Range(-maxInitialVelocity, maxInitialVelocity)
                );
                rb.velocity = randomVelocity;

                // Apply random angular velocity (torque)
                Vector3 randomAngularVelocity = new Vector3(
                    Random.Range(-maxTorque, maxTorque),
                    Random.Range(-maxTorque, maxTorque),
                    Random.Range(-maxTorque, maxTorque)
                );
                rb.angularVelocity = randomAngularVelocity;

                // Wake up the rigidbody
                rb.WakeUp();
            }
        }
    }

    void CapturePhysicsState(float time)
    {
        PhysicsSimulationState state = new PhysicsSimulationState
        {
            time = time,
            positions = new Vector3[physicsObjects.Length],
            rotations = new Quaternion[physicsObjects.Length],
            velocities = new Vector3[physicsObjects.Length],
            angularVelocities = new Vector3[physicsObjects.Length],
            isColliding = new bool[physicsObjects.Length]
        };

        for (int i = 0; i < physicsObjects.Length; i++)
        {
            GameObject obj = physicsObjects[i];
            Rigidbody rb = obj.GetComponent<Rigidbody>();

            state.positions[i] = obj.transform.position;
            state.rotations[i] = obj.transform.rotation;

            if (rb != null)
            {
                state.velocities[i] = rb.velocity;
                state.angularVelocities[i] = rb.angularVelocity;
                state.isColliding[i] = rb.IsSleeping() ? false : CheckCollision(obj);
            }
        }

        // Save or process the captured state
        SavePhysicsState(state);
    }

    bool CheckCollision(GameObject obj)
    {
        // Check if this object is colliding with any other physics objects
        Collider objCollider = obj.GetComponent<Collider>();
        if (objCollider == null) return false;

        foreach (GameObject otherObj in physicsObjects)
        {
            if (otherObj != obj)
            {
                Collider otherCollider = otherObj.GetComponent<Collider>();
                if (otherCollider != null)
                {
                    // Simple overlap check (in practice, you'd use more sophisticated collision detection)
                    if (IsOverlapping(objCollider, otherCollider))
                    {
                        return true;
                    }
                }
            }
        }

        return false;
    }

    bool IsOverlapping(Collider col1, Collider col2)
    {
        // Simple bounding box overlap check
        Bounds bounds1 = col1.bounds;
        Bounds bounds2 = col2.bounds;

        return bounds1.Intersects(bounds2);
    }

    void SavePhysicsState(PhysicsSimulationState state)
    {
        // In a real implementation, you would save this state to a file or database
        // For now, just log some information
        Debug.Log($"Captured physics state at time {state.time:F3}s, " +
                 $"Objects: {state.positions.Length}, " +
                 $"Collisions: {System.Linq.Enumerable.Count(state.isColliding, x => x)}");
    }

    void OnDisable()
    {
        // Reset gravity to default
        Physics.gravity = new Vector3(0, -9.81f, 0);
    }
}
```

## Data Annotation and Labeling

### Automated Annotation Systems

Unity can generate perfect annotations for synthetic data:

```csharp
using UnityEngine;
using System.Collections.Generic;
using System.IO;
using Newtonsoft.Json;

public class AutomatedAnnotationSystem : MonoBehaviour
{
    [Header("Annotation Configuration")]
    public bool generateBoundingBoxes = true;
    public bool generateSegmentation = true;
    public bool generateKeypoints = true;
    public bool generate3DPose = true;

    [Header("Output Configuration")]
    public string annotationOutputDirectory = "Annotations";
    public string annotationFormat = "coco"; // Options: coco, pascal_voc, yolo, etc.

    [Header("Object Configuration")]
    public GameObject[] annotatedObjects;
    public string[] objectCategories;

    [Header("Camera Configuration")]
    public Camera annotationCamera;

    private int currentImageId = 0;
    private int currentAnnotationId = 0;

    [System.Serializable]
    public class CocoAnnotation
    {
        public int id;
        public int image_id;
        public int category_id;
        public float[] bbox; // [x, y, width, height]
        public float[] segmentation; // Polygon points flattened
        public float area;
        public bool iscrowd = false;
        public float[] keypoints; // [x1, y1, v1, x2, y2, v2, ...] where v is visibility
        public int num_keypoints;
    }

    [System.Serializable]
    public class CocoImage
    {
        public int id;
        public string file_name;
        public int width;
        public int height;
        public int depth = 3;
    }

    [System.Serializable]
    public class CocoCategory
    {
        public int id;
        public string name;
        public string supercategory = "object";
    }

    [System.Serializable]
    public class CocoDataset
    {
        public string info = "Synthetic dataset generated with Unity";
        public List<CocoCategory> categories = new List<CocoCategory>();
        public List<CocoImage> images = new List<CocoImage>();
        public List<CocoAnnotation> annotations = new List<CocoAnnotation>();
    }

    private CocoDataset dataset;

    void Start()
    {
        InitializeAnnotationSystem();
    }

    void InitializeAnnotationSystem()
    {
        dataset = new CocoDataset();

        // Initialize categories
        for (int i = 0; i < objectCategories.Length; i++)
        {
            CocoCategory category = new CocoCategory
            {
                id = i + 1, // COCO uses 1-based indexing
                name = objectCategories[i]
            };
            dataset.categories.Add(category);
        }
    }

    public void GenerateAnnotationsForCurrentFrame(string imageName)
    {
        CocoImage image = new CocoImage
        {
            id = currentImageId++,
            file_name = imageName,
            width = Screen.width,
            height = Screen.height
        };

        dataset.images.Add(image);

        // Generate annotations for each object
        foreach (GameObject obj in annotatedObjects)
        {
            if (IsObjectVisibleInCamera(obj, annotationCamera))
            {
                GenerateObjectAnnotation(obj, image.id);
            }
        }

        // Save annotations periodically
        if (currentImageId % 100 == 0) // Save every 100 images
        {
            SaveAnnotations();
        }
    }

    void GenerateObjectAnnotation(GameObject obj, int imageId)
    {
        CocoAnnotation annotation = new CocoAnnotation
        {
            id = currentAnnotationId++,
            image_id = imageId,
            category_id = GetObjectCategoryId(obj)
        };

        // Generate bounding box
        if (generateBoundingBoxes)
        {
            annotation.bbox = GenerateBoundingBox(obj, annotationCamera);
            annotation.area = annotation.bbox[2] * annotation.bbox[3]; // width * height
        }

        // Generate segmentation mask
        if (generateSegmentation)
        {
            annotation.segmentation = GenerateSegmentationPolygon(obj, annotationCamera);
        }

        // Generate keypoints
        if (generateKeypoints)
        {
            var keypointData = GenerateKeypoints(obj, annotationCamera);
            annotation.keypoints = keypointData.keypoints;
            annotation.num_keypoints = keypointData.count;
        }

        dataset.annotations.Add(annotation);
    }

    int GetObjectCategoryId(GameObject obj)
    {
        // Map object name to category ID
        for (int i = 0; i < objectCategories.Length; i++)
        {
            if (obj.name.ToLower().Contains(objectCategories[i].ToLower()))
            {
                return i + 1; // COCO uses 1-based indexing
            }
        }
        return 1; // Default to first category
    }

    float[] GenerateBoundingBox(GameObject obj, Camera cam)
    {
        // Get the renderer's bounds
        Renderer renderer = obj.GetComponent<Renderer>();
        if (renderer == null) return new float[4] { 0, 0, 0, 0 };

        Bounds bounds = renderer.bounds;

        // Project the 8 corners of the bounding box to screen space
        Vector3[] corners = GetBoundingBoxCorners(bounds);
        Vector2 minScreenPos = new Vector2(float.MaxValue, float.MaxValue);
        Vector2 maxScreenPos = new Vector2(float.MinValue, float.MinValue);

        foreach (Vector3 corner in corners)
        {
            Vector3 screenPos = cam.WorldToScreenPoint(corner);

            // Check if point is in front of camera
            if (screenPos.z > 0)
            {
                minScreenPos.x = Mathf.Min(minScreenPos.x, screenPos.x);
                minScreenPos.y = Mathf.Min(minScreenPos.y, screenPos.y);
                maxScreenPos.x = Mathf.Max(maxScreenPos.x, screenPos.x);
                maxScreenPos.y = Mathf.Max(maxScreenPos.y, screenPos.y);
            }
        }

        // Ensure the bounding box is within screen bounds
        minScreenPos.x = Mathf.Max(0, minScreenPos.x);
        minScreenPos.y = Mathf.Max(0, minScreenPos.y);
        maxScreenPos.x = Mathf.Min(Screen.width, maxScreenPos.x);
        maxScreenPos.y = Mathf.Min(Screen.height, maxScreenPos.y);

        // Convert to COCO format: [x, y, width, height]
        // Where (x,y) is the top-left corner
        float x = minScreenPos.x;
        float y = Screen.height - maxScreenPos.y; // Flip Y axis
        float width = maxScreenPos.x - minScreenPos.x;
        float height = maxScreenPos.y - minScreenPos.y;

        return new float[4] { x, y, width, height };
    }

    Vector3[] GetBoundingBoxCorners(Bounds bounds)
    {
        Vector3[] corners = new Vector3[8];
        Vector3 center = bounds.center;
        Vector3 extents = bounds.extents;

        // Generate 8 corners of the bounding box
        corners[0] = new Vector3(center.x - extents.x, center.y - extents.y, center.z - extents.z); // Left bottom back
        corners[1] = new Vector3(center.x + extents.x, center.y - extents.y, center.z - extents.z); // Right bottom back
        corners[2] = new Vector3(center.x - extents.x, center.y + extents.y, center.z - extents.z); // Left top back
        corners[3] = new Vector3(center.x + extents.x, center.y + extents.y, center.z - extents.z); // Right top back
        corners[4] = new Vector3(center.x - extents.x, center.y - extents.y, center.z + extents.z); // Left bottom front
        corners[5] = new Vector3(center.x + extents.x, center.y - extents.y, center.z + extents.z); // Right bottom front
        corners[6] = new Vector3(center.x - extents.x, center.y + extents.y, center.z + extents.z); // Left top front
        corners[7] = new Vector3(center.x + extents.x, center.y + extents.y, center.z + extents.z); // Right top front

        return corners;
    }

    float[] GenerateSegmentationPolygon(GameObject obj, Camera cam)
    {
        // For a simple bounding box segmentation
        // In practice, you'd use more sophisticated methods like instance segmentation rendering
        float[] bbox = GenerateBoundingBox(obj, cam);

        // Create a simple rectangular polygon from the bounding box
        float x = bbox[0], y = bbox[1], width = bbox[2], height = bbox[3];

        // Return polygon as [x1, y1, x2, y2, x3, y3, x4, y4]
        // Order: top-left, top-right, bottom-right, bottom-left
        return new float[] {
            x, y,                    // top-left
            x + width, y,            // top-right
            x + width, y + height,   // bottom-right
            x, y + height            // bottom-left
        };
    }

    (float[] keypoints, int count) GenerateKeypoints(GameObject obj, Camera cam)
    {
        // For this example, we'll generate simple keypoints
        // In practice, you'd have a more sophisticated system with actual joint positions
        List<float> keypoints = new List<float>();
        int count = 0;

        // Example: Generate center point and corner points
        Renderer renderer = obj.GetComponent<Renderer>();
        if (renderer != null)
        {
            Bounds bounds = renderer.bounds;
            Vector3 center = bounds.center;

            // Project center point
            Vector3 centerScreen = cam.WorldToScreenPoint(center);
            if (centerScreen.z > 0) // Point is in front of camera
            {
                float yFlipped = Screen.height - centerScreen.y; // Flip Y axis
                keypoints.AddRange(new float[] { centerScreen.x, yFlipped, 2 }); // x, y, visibility (2 = visible)
                count++;
            }

            // Add corner points
            Vector3[] corners = GetBoundingBoxCorners(bounds);
            foreach (Vector3 corner in corners)
            {
                Vector3 cornerScreen = cam.WorldToScreenPoint(corner);
                if (cornerScreen.z > 0) // Point is in front of camera
                {
                    float yFlipped = Screen.height - cornerScreen.y; // Flip Y axis
                    keypoints.AddRange(new float[] { cornerScreen.x, yFlipped, 2 }); // x, y, visibility (2 = visible)
                    count++;
                }
            }
        }

        return (keypoints.ToArray(), count);
    }

    bool IsObjectVisibleInCamera(GameObject obj, Camera cam)
    {
        Renderer renderer = obj.GetComponent<Renderer>();
        if (renderer == null) return false;

        // Check if the object's bounds are within the camera's view frustum
        Bounds bounds = renderer.bounds;
        return GeometryUtility.TestPlanesAABB(GeometryUtility.CalculateFrustumPlanes(cam), bounds);
    }

    public void SaveAnnotations()
    {
        string json = JsonConvert.SerializeObject(dataset, Newtonsoft.Json.Formatting.Indented);
        string filePath = Path.Combine(annotationOutputDirectory, "annotations.json");

        Directory.CreateDirectory(annotationOutputDirectory);
        File.WriteAllText(filePath, json);

        Debug.Log($"Saved {dataset.annotations.Count} annotations to {filePath}");
    }

    public void ExportAnnotationsForFormat(string format)
    {
        switch (format.ToLower())
        {
            case "coco":
                SaveAnnotations();
                break;
            case "pascal_voc":
                ExportToPascalVOC();
                break;
            case "yolo":
                ExportToYOLO();
                break;
            default:
                Debug.LogError($"Unsupported annotation format: {format}");
                break;
        }
    }

    void ExportToPascalVOC()
    {
        // Implementation for Pascal VOC format
        Debug.Log("Exporting to Pascal VOC format - Implementation needed");
    }

    void ExportToYOLO()
    {
        // Implementation for YOLO format
        Debug.Log("Exporting to YOLO format - Implementation needed");
    }

    void OnDisable()
    {
        // Save any remaining annotations
        if (dataset.annotations.Count > 0)
        {
            SaveAnnotations();
        }
    }
}
```

## Quality Assurance and Validation

### Synthetic Data Quality Metrics

Validating the quality of synthetic data is crucial for ensuring effective model training:

```csharp
using UnityEngine;
using System.Collections.Generic;

public class SyntheticDataQualityAssessor : MonoBehaviour
{
    [Header("Quality Metrics")]
    public bool checkImageQuality = true;
    public bool checkAnnotationAccuracy = true;
    public bool checkDiversity = true;
    public bool checkRealism = true;

    [Header("Image Quality Settings")]
    public float minBrightnessThreshold = 0.1f;
    public float maxBrightnessThreshold = 0.9f;
    public float minContrastThreshold = 0.2f;
    public int minResolutionWidth = 640;
    public int minResolutionHeight = 480;

    [Header("Annotation Quality Settings")]
    public float minBboxAreaRatio = 0.01f; // Minimum 1% of image area
    public float maxBboxAreaRatio = 0.8f;   // Maximum 80% of image area
    public float minKeypointVisibility = 0.5f; // At least 50% keypoints visible

    [Header("Diversity Settings")]
    public int minUniquePoses = 50;
    public int minUniqueLightingConditions = 20;
    public float minPoseVariation = 15.0f; // Minimum rotation difference in degrees

    private List<GeneratedImageData> imageData = new List<GeneratedImageData>();
    private Dictionary<string, List<Quaternion>> poseHistory = new Dictionary<string, List<Quaternion>>();

    [System.Serializable]
    public class GeneratedImageData
    {
        public string filename;
        public Vector2 resolution;
        public float brightness;
        public float contrast;
        public List<BoundingBoxData> bboxes = new List<BoundingBoxData>();
        public List<KeypointData> keypoints = new List<KeypointData>();
        public float[] colorHistogram;
        public float lightingCondition;
        public float realismScore;
    }

    [System.Serializable]
    public class BoundingBoxData
    {
        public string objectName;
        public float[] bbox; // [x, y, width, height]
        public float areaRatio;
        public bool isValid;
    }

    [System.Serializable]
    public class KeypointData
    {
        public string objectName;
        public float[] keypoints; // [x1, y1, v1, x2, y2, v2, ...]
        public int visibleCount;
        public int totalCount;
    }

    public QualityReport AssessDatasetQuality()
    {
        QualityReport report = new QualityReport();

        if (checkImageQuality)
            report.imageQuality = AssessImageQuality();

        if (checkAnnotationAccuracy)
            report.annotationQuality = AssessAnnotationQuality();

        if (checkDiversity)
            report.diversityQuality = AssessDiversity();

        if (checkRealism)
            report.realismQuality = AssessRealism();

        return report;
    }

    ImageQualityMetrics AssessImageQuality()
    {
        ImageQualityMetrics metrics = new ImageQualityMetrics();
        int totalImages = imageData.Count;

        if (totalImages == 0) return metrics;

        int goodBrightness = 0;
        int goodContrast = 0;
        int goodResolution = 0;
        float avgBrightness = 0;
        float avgContrast = 0;

        foreach (var data in imageData)
        {
            if (data.brightness >= minBrightnessThreshold && data.brightness <= maxBrightnessThreshold)
                goodBrightness++;

            if (data.contrast >= minContrastThreshold)
                goodContrast++;

            if (data.resolution.x >= minResolutionWidth && data.resolution.y >= minResolutionHeight)
                goodResolution++;

            avgBrightness += data.brightness;
            avgContrast += data.contrast;
        }

        metrics.percentageGoodBrightness = (float)goodBrightness / totalImages;
        metrics.percentageGoodContrast = (float)goodContrast / totalImages;
        metrics.percentageGoodResolution = (float)goodResolution / totalImages;
        metrics.averageBrightness = avgBrightness / totalImages;
        metrics.averageContrast = avgContrast / totalImages;

        return metrics;
    }

    AnnotationQualityMetrics AssessAnnotationQuality()
    {
        AnnotationQualityMetrics metrics = new AnnotationQualityMetrics();
        int totalAnnotations = 0;
        int validBboxes = 0;
        int validKeypoints = 0;
        float avgAreaRatio = 0;

        foreach (var data in imageData)
        {
            foreach (var bbox in data.bboxes)
            {
                totalAnnotations++;
                if (bbox.areaRatio >= minBboxAreaRatio && bbox.areaRatio <= maxBboxAreaRatio)
                {
                    validBboxes++;
                    avgAreaRatio += bbox.areaRatio;
                }
            }

            foreach (var kp in data.keypoints)
            {
                if (kp.totalCount > 0 && (float)kp.visibleCount / kp.totalCount >= minKeypointVisibility)
                {
                    validKeypoints++;
                }
            }
        }

        if (totalAnnotations > 0)
        {
            metrics.bboxValidityRate = (float)validBboxes / totalAnnotations;
            metrics.averageAreaRatio = avgAreaRatio / validBboxes;
        }

        if (dataImage.Count > 0)
        {
            metrics.keypointValidityRate = (float)validKeypoints / imageData.Count;
        }

        return metrics;
    }

    DiversityMetrics AssessDiversity()
    {
        DiversityMetrics metrics = new DiversityMetrics();

        // Calculate pose diversity
        int uniquePoses = 0;
        foreach (var kvp in poseHistory)
        {
            uniquePoses = Mathf.Max(uniquePoses, GetUniquePoses(kvp.Value));
        }

        metrics.poseDiversity = uniquePoses;
        metrics.hasSufficientPoses = uniquePoses >= minUniquePoses;

        // Calculate lighting diversity (simplified)
        float avgLightingVariation = 0;
        for (int i = 1; i < imageData.Count; i++)
        {
            avgLightingVariation += Mathf.Abs(imageData[i].lightingCondition - imageData[i-1].lightingCondition);
        }

        if (imageData.Count > 1)
        {
            avgLightingVariation /= (imageData.Count - 1);
        }

        metrics.lightingDiversity = avgLightingVariation;
        metrics.hasSufficientLighting = avgLightingVariation > 0.1f; // Arbitrary threshold

        return metrics;
    }

    RealismMetrics AssessRealism()
    {
        RealismMetrics metrics = new RealismMetrics();

        // Calculate average realism score
        float totalRealism = 0;
        foreach (var data in imageData)
        {
            totalRealism += data.realismScore;
        }

        if (imageData.Count > 0)
        {
            metrics.averageRealism = totalRealism / imageData.Count;
            metrics.percentageHighRealism = (float)System.Linq.Enumerable.Count(imageData, d => d.realismScore > 0.7f) / imageData.Count;
        }

        return metrics;
    }

    int GetUniquePoses(List<Quaternion> poses)
    {
        if (poses.Count == 0) return 0;

        int uniqueCount = 1;
        for (int i = 1; i < poses.Count; i++)
        {
            bool isUnique = true;
            for (int j = 0; j < i; j++)
            {
                float angle = Quaternion.Angle(poses[i], poses[j]);
                if (angle < minPoseVariation)
                {
                    isUnique = false;
                    break;
                }
            }
            if (isUnique) uniqueCount++;
        }

        return uniqueCount;
    }

    public void AddImageData(GeneratedImageData data)
    {
        imageData.Add(data);

        // Track pose history for diversity analysis
        foreach (var bbox in data.bboxes)
        {
            if (!poseHistory.ContainsKey(bbox.objectName))
            {
                poseHistory[bbox.objectName] = new List<Quaternion>();
            }

            // In a real implementation, you'd extract the object's rotation
            // For now, we'll use a placeholder
            poseHistory[bbox.objectName].Add(Quaternion.identity);
        }
    }

    [System.Serializable]
    public class QualityReport
    {
        public ImageQualityMetrics imageQuality;
        public AnnotationQualityMetrics annotationQuality;
        public DiversityMetrics diversityQuality;
        public RealismMetrics realismQuality;
    }

    [System.Serializable]
    public class ImageQualityMetrics
    {
        public float percentageGoodBrightness;
        public float percentageGoodContrast;
        public float percentageGoodResolution;
        public float averageBrightness;
        public float averageContrast;
    }

    [System.Serializable]
    public class AnnotationQualityMetrics
    {
        public float bboxValidityRate;
        public float averageAreaRatio;
        public float keypointValidityRate;
    }

    [System.Serializable]
    public class DiversityMetrics
    {
        public int poseDiversity;
        public bool hasSufficientPoses;
        public float lightingDiversity;
        public bool hasSufficientLighting;
    }

    [System.Serializable]
    public class RealismMetrics
    {
        public float averageRealism;
        public float percentageHighRealism;
    }
}
```

## Integration with Machine Learning Frameworks

### Data Pipeline Integration

Connecting synthetic data generation to ML training pipelines:

```csharp
using UnityEngine;
using System.Collections;
using System.IO;
using System.Collections.Generic;

public class MLDataPipelineIntegrator : MonoBehaviour
{
    [Header("ML Framework Configuration")]
    public string framework = "tensorflow"; // Options: tensorflow, pytorch, keras
    public string outputFormat = "tfrecord"; // Options: tfrecord, numpy, json
    public string datasetPath = "D:/ML_Datasets/";

    [Header("Data Split Configuration")]
    public float trainSplit = 0.7f;
    public float validationSplit = 0.15f;
    public float testSplit = 0.15f;
    public bool shuffleData = true;
    public int randomSeed = 42;

    [Header("Preprocessing Configuration")]
    public bool normalizeImages = true;
    public float normalizationRange = 1.0f;
    public bool augmentData = true;
    public bool convertToGrayscale = false;

    [Header("Batch Configuration")]
    public int batchSize = 32;
    public int bufferCount = 1000;

    private List<DataSample> allSamples = new List<DataSample>();
    private System.Random random;

    [System.Serializable]
    public class DataSample
    {
        public string imagePath;
        public string annotationPath;
        public string metadataPath;
        public string split; // train, validation, or test
        public float[] features; // For feature-based ML approaches
    }

    void Start()
    {
        random = new System.Random(randomSeed);
    }

    public void PrepareDatasetForTraining()
    {
        // Organize data into train/validation/test splits
        SplitDataset();

        // Create framework-specific data files
        switch (framework.ToLower())
        {
            case "tensorflow":
                CreateTensorFlowDataset();
                break;
            case "pytorch":
                CreatePyTorchDataset();
                break;
            case "keras":
                CreateKerasDataset();
                break;
            default:
                Debug.LogError($"Unsupported framework: {framework}");
                break;
        }

        Debug.Log($"Dataset preparation completed. Total samples: {allSamples.Count}");
    }

    void SplitDataset()
    {
        if (shuffleData)
        {
            ShuffleList(allSamples);
        }

        int totalSamples = allSamples.Count;
        int trainCount = Mathf.FloorToInt(totalSamples * trainSplit);
        int validationCount = Mathf.FloorToInt(totalSamples * validationSplit);

        for (int i = 0; i < totalSamples; i++)
        {
            if (i < trainCount)
            {
                allSamples[i].split = "train";
            }
            else if (i < trainCount + validationCount)
            {
                allSamples[i].split = "validation";
            }
            else
            {
                allSamples[i].split = "test";
            }
        }
    }

    void CreateTensorFlowDataset()
    {
        // Create TensorFlow-compatible files (TFRecord format)
        string baseDir = Path.Combine(datasetPath, "tensorflow");
        Directory.CreateDirectory(baseDir);

        // Create separate files for each split
        CreateTFRecordFiles("train", baseDir);
        CreateTFRecordFiles("validation", baseDir);
        CreateTFRecordFiles("test", baseDir);

        // Create dataset configuration file
        CreateDatasetConfig(baseDir);
    }

    void CreateTFRecordFiles(string split, string baseDir)
    {
        var splitSamples = allSamples.FindAll(s => s.split == split);
        if (splitSamples.Count == 0) return;

        string filePath = Path.Combine(baseDir, $"{split}.tfrecord");

        // In a real implementation, you would write TFRecord files
        // This requires TensorFlow bindings for Unity or external processing
        Debug.Log($"Creating TFRecord file for {split} split: {filePath}");

        // The actual TFRecord creation would happen here
        // For now, we'll just log the operation
    }

    void CreatePyTorchDataset()
    {
        // Create PyTorch-compatible directory structure
        string baseDir = Path.Combine(datasetPath, "pytorch");
        Directory.CreateDirectory(baseDir);

        // Create train/validation/test directories
        string trainDir = Path.Combine(baseDir, "train");
        string valDir = Path.Combine(baseDir, "validation");
        string testDir = Path.Combine(baseDir, "test");

        Directory.CreateDirectory(trainDir);
        Directory.CreateDirectory(valDir);
        Directory.CreateDirectory(testDir);

        // Copy files to appropriate directories
        foreach (var sample in allSamples)
        {
            string targetDir = sample.split == "train" ? trainDir :
                             sample.split == "validation" ? valDir : testDir;

            // Copy image and annotation files
            if (!string.IsNullOrEmpty(sample.imagePath))
            {
                string targetImagePath = Path.Combine(targetDir, Path.GetFileName(sample.imagePath));
                File.Copy(sample.imagePath, targetImagePath, true);
            }

            if (!string.IsNullOrEmpty(sample.annotationPath))
            {
                string targetAnnotationPath = Path.Combine(targetDir, Path.GetFileName(sample.annotationPath));
                File.Copy(sample.annotationPath, targetAnnotationPath, true);
            }
        }

        Debug.Log($"PyTorch dataset structure created at: {baseDir}");
    }

    void CreateKerasDataset()
    {
        // Keras typically uses the same format as TensorFlow
        CreateTensorFlowDataset();
    }

    void CreateDatasetConfig(string baseDir)
    {
        // Create a configuration file for the dataset
        var config = new
        {
            framework = this.framework,
            output_format = this.outputFormat,
            train_split = this.trainSplit,
            validation_split = this.validationSplit,
            test_split = this.testSplit,
            total_samples = allSamples.Count,
            train_samples = allSamples.FindAll(s => s.split == "train").Count,
            validation_samples = allSamples.FindAll(s => s.split == "validation").Count,
            test_samples = allSamples.FindAll(s => s.split == "test").Count,
            image_width = Screen.width,
            image_height = Screen.height,
            normalization_applied = this.normalizeImages,
            augmentation_applied = this.augmentData
        };

        string configPath = Path.Combine(baseDir, "dataset_config.json");
        string json = JsonUtility.ToJson(config, true);
        File.WriteAllText(configPath, json);

        Debug.Log($"Dataset configuration saved to: {configPath}");
    }

    void ShuffleList<T>(List<T> list)
    {
        for (int i = list.Count - 1; i > 0; i--)
        {
            int j = random.Next(i + 1);
            T temp = list[i];
            list[i] = list[j];
            list[j] = temp;
        }
    }

    public void AddSample(string imagePath, string annotationPath, string metadataPath)
    {
        DataSample sample = new DataSample
        {
            imagePath = imagePath,
            annotationPath = annotationPath,
            metadataPath = metadataPath
        };

        allSamples.Add(sample);
    }

    public void ClearDataset()
    {
        allSamples.Clear();
        poseHistory.Clear();
    }

    private Dictionary<string, List<Quaternion>> poseHistory = new Dictionary<string, List<Quaternion>>();

    public void ExportForTraining(string framework, string outputPath)
    {
        // Set framework and path
        this.framework = framework;
        this.datasetPath = outputPath;

        // Prepare and export dataset
        PrepareDatasetForTraining();
    }
}
```

## Best Practices for Synthetic Data Generation

### 1. Domain Randomization

Apply systematic randomization of visual properties to improve model generalization across different domains.

### 2. Quality Validation

Implement comprehensive quality assessment to ensure synthetic data meets the standards required for effective model training.

### 3. Realism vs. Diversity Trade-off

Balance the realism of generated data with the diversity needed for robust model training.

### 4. Annotation Accuracy

Ensure perfect annotations by leveraging Unity's scene graph and physics information.

### 5. Performance Optimization

Optimize rendering and generation pipelines to maximize data throughput while maintaining quality.

## Common Challenges and Solutions

### Challenge: Domain Gap
**Solution**: Implement domain randomization and gradually introduce real-world characteristics to bridge the gap.

### Challenge: Computational Cost
**Solution**: Use efficient rendering techniques, level-of-detail systems, and distributed generation across multiple machines.

### Challenge: Annotation Complexity
**Solution**: Leverage Unity's built-in scene information to generate perfect annotations automatically.

### Challenge: Data Bias
**Solution**: Implement systematic variation strategies and monitor dataset statistics for potential biases.

## Summary

Synthetic data generation in Unity provides a powerful approach to creating large-scale, perfectly annotated datasets for training computer vision models. By leveraging Unity's rendering capabilities, physics simulation, and scene management, developers can generate diverse and realistic datasets that improve model performance while reducing the cost and time associated with real-world data collection. The key to successful synthetic data generation lies in balancing visual realism with diversity, implementing robust annotation systems, and validating data quality through comprehensive assessment metrics. As machine learning models become more sophisticated and require larger, more diverse datasets, synthetic data generation will continue to play an increasingly important role in the development of vision-based robotic systems.

## References

1. Shapira, O., et al. (2019). "Synthetic Data Generation for End-to-End Thermal Infrared Tracking." arXiv preprint arXiv:1909.00695.
2. Tremblay, J., et al. (2018). "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization." IEEE Conference on Computer Vision and Pattern Recognition Workshops.
3. Unity Technologies. (2023). "Unity Computer Vision Package Documentation." Unity Technologies.

## Exercises

1. Implement a synthetic data generator for a specific object detection task
2. Create a domain randomization system that systematically varies visual properties
3. Develop an automated annotation system for a custom robotic vision task
4. Design a quality assessment pipeline for synthetic data validation